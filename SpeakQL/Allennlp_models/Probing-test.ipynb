{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x112615770>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict, Optional, cast\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField, ArrayField, MetadataField, ListField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.batch import Batch\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding, TokenEmbedder\n",
    "from allennlp.modules.token_embedders.pretrained_transformer_embedder import PretrainedTransformerEmbedder\n",
    "from allennlp.modules.token_embedders.pretrained_transformer_mismatched_embedder import PretrainedTransformerMismatchedEmbedder\n",
    "# from allennlp.modules.seq2seq_encoders.multi_head_self_attention import MultiHeadSelfAttention\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n",
    "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
    "from allennlp.modules.matrix_attention.cosine_matrix_attention import CosineMatrixAttention\n",
    "from allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention\n",
    "\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions, ConditionalRandomField\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits, \\\n",
    "    get_device_of, masked_softmax, weighted_sum, \\\n",
    "    get_mask_from_sequence_lengths, get_lengths_from_binary_sequence_mask, tensors_equal\n",
    "\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy, MeanAbsoluteError, Average\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.dataloader import DataLoader, PyTorchDataLoader\n",
    "from allennlp.training.trainer import GradientDescentTrainer\n",
    "# from allennlp.predictors import Predictor, Seq2SeqPredictor, SimpleSeq2SeqPredictor, SentenceTaggerPredictor\n",
    "from allennlp.predictors import Predictor, SentenceTaggerPredictor\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.util import JsonDict, sanitize\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "from allennlp_models.generation.predictors import Seq2SeqPredictor\n",
    "from allennlp_models.generation.models.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp_models.generation.modules.seq_decoders.seq_decoder import SeqDecoder\n",
    "from allennlp_models.generation.modules.decoder_nets.decoder_net import DecoderNet\n",
    "\n",
    "\n",
    "# from spacy.tokenizer import Tokenizer as SpacyTokenizer\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import ShortTermFeatures\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from inspect import signature\n",
    "import warnings\n",
    "import pickle\n",
    "from copy import copy, deepcopy\n",
    "from overrides import overrides\n",
    "import importlib\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import editdistance\n",
    "from copy import copy, deepcopy\n",
    "import random\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig, BertTokenizer\n",
    "\n",
    "from utils.spider import process_sql, evaluation\n",
    "from utils.schema_gnn.spider_utils import Table, TableColumn, read_dataset_schema\n",
    "from utils.misc_utils import Load_CMU_Dict, WordPronDist, WordPronSimilarity, ConstructWordSimMatrix\n",
    "\n",
    "import dataset_readers\n",
    "import models\n",
    "import predictors\n",
    "\n",
    "from dataset_readers.reader_utils import extractAudioFeatures, extractAudioFeatures_NoPooling, \\\n",
    "    extractRawAudios, extractAudioFeatures_NoPooling_Wav2vec, \\\n",
    "    dbToTokens, dbToTokensWithColumnIndexes, dbToTokensWithAddCells, \\\n",
    "    read_DB, Get_align_tags, load_DB_content, collect_DB_toks_dict, text_cell_to_toks\n",
    "# from modules.encoder import SpeakQLEncoder, SpeakQLEncoderV1, SpeakQLEncoder_Gated_Fusion\n",
    "from modules.encoder import SpeakQLEncoder, SpeakQLEncoderV1\n",
    "# from models.reranker import SpiderASRRerankerV0, SpiderASRRerankerV1, SpiderASRRerankerV2, SpiderASRReranker_Siamese\n",
    "# from predictors.reranker_predictor import SpiderASRRerankerPredictor, SpiderASRRerankerPredictor_Siamese\n",
    "\n",
    "from dataset_readers import SpiderASRRerankerReaderV2_Siamese_Combined\n",
    "from dataset_readers import SpiderASRRewriterReader_Seq2seq_Combined, \\\n",
    "    SpiderASRRewriterReader_Tagger_Combined, SpiderASRRewriterReader_ILM_Combined\n",
    "from models import SpiderASRRewriter_Tagger_Combined, SpiderASRRewriter_ILM_Combined, \\\n",
    "    SpiderASRRewriter_Seq2seq_Combined, \\\n",
    "    SpiderASRRewriter_Tagger_Combined_new, SpiderASRRewriter_ILM_Combined_new\n",
    "from predictors import SpiderASRRewriterPredictor_Tagger, SpiderASRRewriterPredictor_ILM, SpiderASRRewriterPredictor_Seq2seq\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(dataset_readers)\n",
    "# from dataset_readers import SpiderASRRewriterReader_ILM_Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset_reader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to change config version here, if dataset reader is unchanged \n",
    "full_config = Params.from_file('train_configs/rewriter_2.29.0.0i.jsonnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'databases_dir': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database',\n",
       " 'dataset_dir': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my',\n",
       " 'db_cells_in_bracket': True,\n",
       " 'db_tok2phs_dict_path': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/db/db_tok2phs.json',\n",
       " 'include_align_tags': True,\n",
       " 'include_gold_rewrite_seq': True,\n",
       " 'max_sequence_len': 300,\n",
       " 'ph_token_indexers': {'phonemes': {'namespace': 'phonemes',\n",
       "   'type': 'single_id'}},\n",
       " 'pronun_dict_path': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/CMUdict/cmudict-0.7b.txt',\n",
       " 'specify_full_path': False,\n",
       " 'src_token_indexers': {'bert': {'model_name': 'facebook/bart-base',\n",
       "   'type': 'pretrained_transformer_mismatched'},\n",
       "  'char': {'min_padding_length': 5,\n",
       "   'namespace': 'token_characters',\n",
       "   'type': 'characters'}},\n",
       " 'tabert_model_path': '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/pretrained-models/tabert_base_k1/model.bin',\n",
       " 'tables_json_fname': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json',\n",
       " 'tgt_token_indexers': {'tgt_tokens': {'namespace': 'tgt_tokens',\n",
       "   'type': 'single_id'}},\n",
       " 'type': 'spider_ASR_rewriter_reader_ILM_comb_new',\n",
       " 'use_db_cells': 'K=5',\n",
       " 'use_phoneme_inputs': False,\n",
       " 'use_phoneme_labels': True,\n",
       " 'use_tabert': False,\n",
       " 'use_tagger_prediction': False,\n",
       " 'aux_probes': {'utter_mention_schema': True,\n",
       "  'schema_dir_mentioned': True,\n",
       "  'schema_indir_mentioned': True}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsreader_config = deepcopy(full_config['dataset_reader'])\n",
    "dsreader_config['databases_dir'] = \"/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database\"\n",
    "dsreader_config['dataset_dir'] = \"/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my\"\n",
    "dsreader_config['tables_json_fname'] = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "dsreader_config['tabert_model_path'] = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/pretrained-models/tabert_base_k1/model.bin'\n",
    "dsreader_config['pronun_dict_path'] = \"/Users/mac/Desktop/syt/Deep-Learning/Dataset/CMUdict/cmudict-0.7b.txt\"\n",
    "dsreader_config['db_tok2phs_dict_path'] = \"/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/db/db_tok2phs.json\"\n",
    "dsreader_config['aux_probes'] = {\n",
    "    'utter_mention_schema': True,\n",
    "    'schema_dir_mentioned': True,\n",
    "    'schema_indir_mentioned': True,\n",
    "}\n",
    "dsreader_config.as_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (empirical memory concern) use ~1000 samples, will give ~5000 datapoints for probing test for R1 \n",
    "dsreader_config['cands_limit'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CMU Dict done: 133854 entries, 125074 words, 113745 prons\n",
      "Loading db_tok2phs Dict done: 102186 entries, 102186 words, 91693 prons\n",
      "Joint word2pron size = 202111\n",
      "[('hellenizing', {('HH', 'EH', 'L', 'AH', 'N', 'AY', 'Z', 'IH', 'NG')}), ('kinsella', {('K', 'IY', 'N', 'S', 'EH', 'L', 'AH')}), ('matsch', {('M', 'AE', 'CH')}), ('odonnel', {('OW', 'D', 'AA', 'N', 'AH', 'L')}), ('quads', {('K', 'W', 'AA', 'D', 'Z')}), (\"sears'\", {('S', 'IH', 'R', 'Z')})]\n"
     ]
    }
   ],
   "source": [
    "test_dataset_reader = DatasetReader.from_params(deepcopy(dsreader_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fe296fbc964c538d604f988702e892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading literals failed: wta_1::players\n",
      "['first_name', 'last_name', 'hand', 'country_code']\n",
      "Could not decode to UTF-8 column 'last_name' with text 'Treyes Albarrac��N'\n",
      "Question OOV: 65, [('doch', ['D', 'AA', 'K']), ('youll', ['Y', 'AW', 'L']), ('republik', ['R', 'IH', 'P', 'AH', 'B', 'L', 'IH', 'K']), ('everdeen', ['EH', 'V', 'ER', 'D', 'IY', 'N']), ('cdo', ['S', 'IY', 'D', 'IY', 'OW']), ('teoh', ['T', 'IH', 'OW']), ('roomba', ['R', 'UW', 'M', 'B', 'AX']), ('citis', ['S', 'AY', 'DX', 'IH', 'S']), ('zahren', ['Z', 'AA', 'R', 'AX', 'N']), ('lexx', ['L', 'EH', 'K', 'S'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = test_dataset_reader.read('test')\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CMU Dict done: 133854 entries, 125074 words, 113745 prons\n",
      "Loading db_tok2phs Dict done: 102186 entries, 102186 words, 91693 prons\n",
      "Joint word2pron size = 202111\n",
      "[('hellenizing', {('HH', 'EH', 'L', 'AH', 'N', 'AY', 'Z', 'IH', 'NG')}), ('kinsella', {('K', 'IY', 'N', 'S', 'EH', 'L', 'AH')}), ('matsch', {('M', 'AE', 'CH')}), ('odonnel', {('OW', 'D', 'AA', 'N', 'AH', 'L')}), ('quads', {('K', 'W', 'AA', 'D', 'Z')}), (\"sears'\", {('S', 'IH', 'R', 'Z')})]\n"
     ]
    }
   ],
   "source": [
    "# limit train set size to fit into memory; 7000 samples, ~35000 datapoints for R1 \n",
    "dsreader_config['cands_limit'] = 1\n",
    "train_dataset_reader = DatasetReader.from_params(deepcopy(dsreader_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to set random seed to fix this sampling \n",
    "random.seed(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08a28623fe74415bfe5fd3e5163f3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading literals failed: wta_1::players\n",
      "['first_name', 'last_name', 'hand', 'country_code']\n",
      "Could not decode to UTF-8 column 'last_name' with text 'Treyes Albarrac��N'\n",
      "Question OOV: 587, [('sendin', ['S', 'EH', 'N', 'D', 'IH', 'N']), ('fea', ['F', 'IY']), ('conserves', ['K', 'AX', 'N', 'S', 'ER', 'V', 'Z']), ('nancie', ['N', 'AE', 'N', 'S', 'IY']), ('jins', ['JH', 'IH', 'N', 'Z']), ('alis', ['AA', 'L', 'IY', 'Z']), ('roselyn', ['R', 'OW', 'Z', 'L', 'IH', 'N']), ('louiss', ['L', 'UW', 'IH', 'S']), ('statuses', ['S', 'T', 'DX', 'AX', 'S', 'Z']), ('illy', ['IH', 'L', 'IY'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset_reader.read('train')\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/dev.json'\n",
    "# with open(original_dev_path, 'r') as f:\n",
    "#     original_dev_dataset = json.load(f)\n",
    "# len(original_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/train/train_rewriter+phonemes.json'\n",
    "with open(full_train_path, 'r') as f:\n",
    "    full_train_dataset = json.load(f)\n",
    "len(full_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dev_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/dev_rewriter(full)+phonemes.json'\n",
    "with open(full_dev_path, 'r') as f:\n",
    "    full_dev_dataset = json.load(f)\n",
    "len(full_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance', 'alignment_span_pairs', 'alignment_text_pairs', 'rewriter_tags', 'rewriter_edits', 'token_phonemes', 'token_phoneme_spans'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dev_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': <allennlp.data.fields.text_field.TextField at 0x152908410>,\n",
       " 'text_mask': <allennlp.data.fields.array_field.ArrayField at 0x152908460>,\n",
       " 'schema_mask': <allennlp.data.fields.array_field.ArrayField at 0x1529084b0>,\n",
       " 'schema_column_ids': <allennlp.data.fields.array_field.ArrayField at 0x152908550>,\n",
       " 'audio_feats': <allennlp.data.fields.list_field.ListField at 0x1528f3990>,\n",
       " 'audio_mask': <allennlp.data.fields.array_field.ArrayField at 0x152908500>,\n",
       " 'phoneme_multilabels': <allennlp.data.fields.list_field.ListField at 0x1528eff50>,\n",
       " 'phoneme_labels': <allennlp.data.fields.list_field.ListField at 0x152903e10>,\n",
       " 'phoneme_label_mask': <allennlp.data.fields.list_field.ListField at 0x152903e90>,\n",
       " 'utter_mention_schema_labels': <allennlp.data.fields.array_field.ArrayField at 0x152916c30>,\n",
       " 'schema_dir_mentioned_labels': <allennlp.data.fields.array_field.ArrayField at 0x152916c80>,\n",
       " 'schema_indir_mentioned_labels': <allennlp.data.fields.array_field.ArrayField at 0x152916cd0>,\n",
       " 'align_tags': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x1570ee830>,\n",
       " 'ref_att_map': <allennlp.data.fields.array_field.ArrayField at 0x152916d70>,\n",
       " 'source_to_target': <allennlp.data.fields.namespace_swapping_field.NamespaceSwappingField at 0x152916d20>,\n",
       " 'rewriter_tags': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x1570ee1d0>,\n",
       " 'rewrite_seq': <allennlp.data.fields.text_field.TextField at 0x152916dc0>,\n",
       " 'source_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x152916e60>,\n",
       " 'target_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x152916eb0>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x1528efed0>}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_instance = test_dataset[100]\n",
    "_test_instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['original_id',\n",
       " 'text_len',\n",
       " 'schema_len',\n",
       " 'concat_len',\n",
       " 'text_tokens',\n",
       " 'schema_tokens',\n",
       " 'concat_tokens',\n",
       " 'source_tokens',\n",
       " 'target_tokens',\n",
       " 'rewrite_seq_len']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_test_instance.fields['metadata'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41,  8, 42, 43])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_instance.fields['target_token_ids'].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[airlines, [ANS]]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_instance.fields['metadata']['target_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_instance.fields['metadata']['source_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, (68,))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_test_instance.fields['metadata']['source_tokens']), _test_instance.fields['source_token_ids'].array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._start_index: 3, @start@\n",
      "self._end_index: 4, @end@\n",
      "self._pad_index: 0, @@PADDING@@\n"
     ]
    }
   ],
   "source": [
    "MODEL_VER = \"2.29.0.0i\"\n",
    "\n",
    "# tagger_ILM_model = Model.from_archive('runs/2.0.1/model.tar.gz')\n",
    "ILM_model = Model.from_archive(f'runs/{MODEL_VER}/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct predictor \n",
    "# Just using train_reader; shouldn't have problem since the dataset_reader here is not really used in this code \n",
    "predictor = SpiderASRRewriterPredictor_ILM(model=ILM_model,\n",
    "                                           dataset_reader=train_dataset_reader)\n",
    "predictor.set_save_intermediate(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test code \n",
    "predictor_output = predictor.predict_instance(_test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['question', 'original_id', 'rewriter_tags', 'align_tags', 'rewrite_seq_prediction', 'rewrite_seq_prediction_cands', 'rewrite_seq_NLL', 'rewrite_seq_prediction_intermediates'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['phoneme_attention_map_0', 'phoneme_attention_out_0', 'phoneme_attention_out_with_residual_0', 'encoder_representation_0'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_output['rewrite_seq_prediction_intermediates']['encoder'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word_embeddings', 'audio_feats_encoded', 'phoneme_tag_embeddings', 'encoder_seq_representation_with_tag'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_output['rewrite_seq_prediction_intermediates']['rewriter_main'].keys()\n",
    "# \"tabert_embedding\" actually means token embedding, not necessarily tabert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictor_output['rewrite_seq_prediction_intermediates']['decoder']['state_decoder_hidden:step_0']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_repr_encoded = predictor_output['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag']\n",
    "# np.array(tokens_repr_encoded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_feats_encoded = predictor_output['rewrite_seq_prediction_intermediates']['rewriter_main']['audio_feats_encoded']\n",
    "# np.array(audio_feats_encoded).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _batch = Batch([_test_instance]).as_tensor_dict()\n",
    "# _text_mask = _batch['text_mask']\n",
    "# _schema_mask = _batch['schema_mask']\n",
    "# _text_mask, _schema_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full prediction (can't do)\n",
    "- taking too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pred_outputs = []\n",
    "# for _inst in tqdm(test_dataset):\n",
    "#     predictor_output = predictor.predict_instance(_inst)\n",
    "#     all_pred_outputs.append(predictor_output)\n",
    "\n",
    "# print(len(all_pred_outputs))\n",
    "# print(list(all_pred_outputs[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(clf, X, y, res_dict=None):\n",
    "    p = clf.predict_proba(X)\n",
    "    preds = [np.argmax(_p) for _p in p]\n",
    "\n",
    "    corr = 0\n",
    "    corr_pos = 0\n",
    "    all_pred_pos = 0\n",
    "    all_true_pos = 0\n",
    "    for _pred, _y in zip(preds, y):\n",
    "        corr += (_pred == _y)\n",
    "        corr_pos += (_pred > 0 and _y > 0 and _pred == _y)\n",
    "        all_pred_pos += (_pred > 0)\n",
    "        all_true_pos += (_y > 0)\n",
    "\n",
    "    acc = corr / len(y)\n",
    "    prec = corr_pos / (all_pred_pos + 1e-9)\n",
    "    rec = corr_pos / (all_true_pos + 1e-9)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "    pos_prop = sum(y) / len(y)\n",
    "\n",
    "    print(f'Positive prop: {pos_prop:.4f}')\n",
    "    print(f'Correct positives: {corr_pos}')\n",
    "    print(f'All pred positives: {all_pred_pos}')\n",
    "    print(f'All true positives: {all_true_pos}')\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print(f'Precision: {prec:.4f}')\n",
    "    print(f'Recall: {rec:.4f}')\n",
    "    print(f'F1: {f1:.4f}')\n",
    "    \n",
    "    if res_dict is not None:\n",
    "        assert isinstance(res_dict, dict)\n",
    "        res_dict['pos_prop'] = pos_prop\n",
    "        res_dict['corr_pos'] = corr_pos\n",
    "        res_dict['all_pred_pos'] = all_pred_pos\n",
    "        res_dict['all_true_pos'] = all_true_pos\n",
    "        res_dict['acc'] = acc\n",
    "        res_dict['prec'] = prec\n",
    "        res_dict['rec'] = rec\n",
    "        res_dict['f1'] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_analysis_utter(clf, X, y, err_output_path=None):\n",
    "    p = clf.predict_proba(X)\n",
    "    preds = [np.argmax(_p) for _p in p]\n",
    "\n",
    "    incorr_indices = []\n",
    "    for i, (_p, _x, _y) in enumerate(zip(p, X, y)):\n",
    "        if _y != np.argmax(_p):\n",
    "            incorr_indices.append(i)\n",
    "    print(len(incorr_indices))\n",
    "\n",
    "    _ptr = 0\n",
    "    err_output = ''\n",
    "    for o_id, _insts in test_o_id2instances.items():\n",
    "    #     _cands = full_dev_dataset[o_id]\n",
    "        _sample_incorr = False\n",
    "        for _inst in _insts:\n",
    "            _incorr = False\n",
    "            _marked_toks = []\n",
    "            for _tok in _inst['metadata']['text_tokens']:\n",
    "                if _ptr in incorr_indices:\n",
    "                    _marked_toks.append(f'<{_tok}:{p[_ptr][1]:.2f}>')\n",
    "                    _incorr = True\n",
    "                    _sample_incorr = True\n",
    "                else:\n",
    "                    _marked_toks.append(_tok)\n",
    "\n",
    "                _ptr += 1\n",
    "\n",
    "            if _incorr:\n",
    "                err_output += ' '.join(_marked_toks) + '\\n'\n",
    "\n",
    "        if _sample_incorr:\n",
    "            err_output += ' '.join(_insts[0]['metadata']['schema_tokens']) + '\\n\\n'\n",
    "\n",
    "    if err_output_path is None:\n",
    "        print(err_output)\n",
    "    else:\n",
    "        with open(err_output_path, 'w') as f:\n",
    "            f.write(err_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_analysis_schema(clf, X, y, err_output_path=None):\n",
    "    p = clf.predict_proba(X)\n",
    "    preds = [np.argmax(_p) for _p in p]\n",
    "\n",
    "    incorr_indices = []\n",
    "    for i, (_p, _x, _y) in enumerate(zip(p, X, y)):\n",
    "        if _y != np.argmax(_p):\n",
    "            incorr_indices.append(i)\n",
    "    print(len(incorr_indices))\n",
    "\n",
    "    _ptr = 0\n",
    "    err_output = ''\n",
    "    for o_id, _insts in test_o_id2instances.items():\n",
    "    #     _cands = full_dev_dataset[o_id]\n",
    "        _skip = False\n",
    "        for _inst in _insts:\n",
    "            _incorr = False\n",
    "            _marked_toks = []\n",
    "            for _tok in _inst['metadata']['schema_tokens']:\n",
    "                if _skip:\n",
    "                    pass\n",
    "                elif _ptr in incorr_indices:\n",
    "                    _marked_toks.append(f'<{_tok}:{p[_ptr][1]:.2f}>')\n",
    "                    _incorr = True\n",
    "                else:\n",
    "                    _marked_toks.append(_tok)\n",
    "\n",
    "                _ptr += 1\n",
    "\n",
    "            if _incorr:\n",
    "                _skip = True   # when find 1 wrong cand, skip others \n",
    "                err_output += f'({o_id}) ' + \\\n",
    "                      ' '.join(_inst['metadata']['text_tokens']) + ' || ' + \\\n",
    "                      ' '.join(_marked_toks) + '\\n'\n",
    "\n",
    "    if err_output_path is None:\n",
    "        print(err_output)\n",
    "    else:\n",
    "        with open(err_output_path, 'w') as f:\n",
    "            f.write(err_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather o_id2instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather instances by o_id \n",
    "train_o_id2instances = defaultdict(list)\n",
    "test_o_id2instances = defaultdict(list)\n",
    "# o_id2preds = defaultdict(list)\n",
    "\n",
    "for _inst in train_dataset:\n",
    "    o_id = _inst['metadata']['original_id']\n",
    "    train_o_id2instances[o_id].append(_inst)\n",
    "for _inst in test_dataset:\n",
    "    o_id = _inst['metadata']['original_id']\n",
    "    test_o_id2instances[o_id].append(_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_o_id2instances.keys()), len(test_o_id2instances.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A - token correctness (utterence-only)\n",
    "- A.1: getting labels by gold rewriter tags, KEEP = 0, DEL = 1, EDIT = 2\n",
    "<!-- - A.2: getting labels by checking the alignment, if the token exists in counterpart then it’s correct, otherwise incorrect (doesn't seem so necessary) -->\n",
    "- Results (2.12.3.3): \n",
    "    - pos% = 0.3533\n",
    "    - Train F1 = 1.0000\n",
    "    - Test F1 = 0.9996\n",
    "- Results (2.18.2.2): \n",
    "    - pos% = 0.3480\n",
    "    - Train F1 = 1.0000\n",
    "    - Test F1 = 0.9969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['original_id',\n",
       " 'text_len',\n",
       " 'schema_len',\n",
       " 'text_tokens',\n",
       " 'schema_tokens',\n",
       " 'rewrite_seq_len']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_test_instance['metadata'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for o_id, _insts in test_o_id2instances.items():\n",
    "#     _cands = full_dev_dataset[o_id]\n",
    "#     _preds = o_id2preds[o_id]\n",
    "#     assert len(_insts) == len(_cands) == len(_preds)\n",
    "#     for _inst, _cand, _pred in zip(_insts, _cands, _preds):\n",
    "#         assert ' '.join(_inst['metadata']['text_tokens']) == ' '.join(_cand['question_toks']) == _pred['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfd6a550baf4fd3b7173b6b7c455802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e335d03ccb2a4e2d932d766e643c920b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather data \n",
    "train_samples_A = []\n",
    "test_samples_A = []\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "#     _preds = o_id2preds[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _utter_encodings = _token_encodings[:_text_len]\n",
    "        \n",
    "        # _labels = [1 if _t == 'O-KEEP' else 0 for _t in _cand['rewriter_tags']]\n",
    "        _labels = []\n",
    "        for _t in _cand['rewriter_tags']:\n",
    "            if _t == 'O-KEEP':\n",
    "                _labels.append(0)\n",
    "            elif _t.endswith('DEL'):\n",
    "                _labels.append(1)\n",
    "            elif _t.endswith('EDIT'):\n",
    "                _labels.append(2)\n",
    "            else:\n",
    "                raise ValueError(_t)\n",
    "        \n",
    "        train_samples_A.extend(list(zip(_utter_encodings, _labels)))\n",
    "\n",
    "\n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "#     _preds = o_id2preds[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _utter_encodings = _token_encodings[:_text_len]\n",
    "        \n",
    "        # _labels = [1 if _t == 'O-KEEP' else 0 for _t in _cand['rewriter_tags']]\n",
    "        _labels = []\n",
    "        for _t in _cand['rewriter_tags']:\n",
    "            if _t == 'O-KEEP':\n",
    "                _labels.append(0)\n",
    "            elif _t.endswith('DEL'):\n",
    "                _labels.append(1)\n",
    "            elif _t.endswith('EDIT'):\n",
    "                _labels.append(2)\n",
    "            else:\n",
    "                raise ValueError(_t)\n",
    "        \n",
    "        test_samples_A.extend(list(zip(_utter_encodings, _labels)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85292, 41776)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples_A), len(test_samples_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "train_X = [s[0] for s in train_samples_A]\n",
    "train_y = [s[1] for s in train_samples_A]\n",
    "test_X = [s[0] for s in test_samples_A]\n",
    "test_y = [s[1] for s in test_samples_A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.pyenv/versions/3.7.5/envs/py3.7-speakql/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive prop: 0.3480\n",
      "Correct positives: 15972\n",
      "All pred positives: 15972\n",
      "All true positives: 15972\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "eval_results(clf, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive prop: 0.3500\n",
      "Correct positives: 7814\n",
      "All pred positives: 7881\n",
      "All true positives: 7886\n",
      "Accuracy: 0.9983\n",
      "Precision: 0.9915\n",
      "Recall: 0.9909\n",
      "F1: 0.9912\n"
     ]
    }
   ],
   "source": [
    "eval_results(clf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B - token mentioning schema (utterence-only)\n",
    "- getting labels by lemma match with schema toks\n",
    "- Result (2.12.3.3):\n",
    "    - pos% = 0.2636\n",
    "    - F1 = 0.8105\n",
    "- Result (2.18.2.2):\n",
    "    - pos% = 0.2573\n",
    "    - Train F1 = 0.8410\n",
    "    - F1 = 0.7899\n",
    "- Result (2.23.0.1) (added head):\n",
    "    - pos% = 0.2573\n",
    "    - Train F1 = 0.9792\n",
    "    - F1 = 0.9223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc716a3f55394400b122e3fa445f5e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0972d504aa2490ca89ecfc03c561040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather data (X for schema toks (can use A), y for task-B)\n",
    "\n",
    "train_labels_B = []\n",
    "test_labels_B = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _metadata = _inst['metadata']\n",
    "#         _text_len = _metadata['text_len']\n",
    "#         _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "#         _utter_encodings = _token_encodings[:_text_len]\n",
    "        \n",
    "        _labels = []\n",
    "        \n",
    "        _utter_tokens_stem = [stemmer.stem(_t) for _t in _metadata['text_tokens']]\n",
    "        _schema_tokens_stem = [stemmer.stem(_t) for _t in _metadata['schema_tokens'] if _t not in string.punctuation]\n",
    "        for _ut in _utter_tokens_stem:\n",
    "            if _ut in _schema_tokens_stem:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "#         samples2.extend(list(zip(_utter_encodings, _labels)))\n",
    "        train_labels_B.extend(_labels)\n",
    "\n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _metadata = _inst['metadata']\n",
    "#         _text_len = _metadata['text_len']\n",
    "#         _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "#         _utter_encodings = _token_encodings[:_text_len]\n",
    "        \n",
    "        _labels = []\n",
    "        \n",
    "        _utter_tokens_stem = [stemmer.stem(_t) for _t in _metadata['text_tokens']]\n",
    "        _schema_tokens_stem = [stemmer.stem(_t) for _t in _metadata['schema_tokens'] if _t not in string.punctuation]\n",
    "        for _ut in _utter_tokens_stem:\n",
    "            if _ut in _schema_tokens_stem:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "#         samples2.extend(list(zip(_utter_encodings, _labels)))\n",
    "        test_labels_B.extend(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85292, 41776)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_B), len(test_labels_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert [s[0] for s in samples_B] == X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "train_X = [s[0] for s in train_samples_A]\n",
    "train_y = train_labels_B\n",
    "test_X = [s[0] for s in test_samples_A]\n",
    "test_y = test_labels_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.pyenv/versions/3.7.5/envs/py3.7-speakql/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive prop: 0.2573\n",
      "Correct positives: 21420\n",
      "All pred positives: 21810\n",
      "All true positives: 21942\n",
      "Accuracy: 0.9893\n",
      "Precision: 0.9821\n",
      "Recall: 0.9762\n",
      "F1: 0.9792\n"
     ]
    }
   ],
   "source": [
    "eval_results(clf, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive prop: 0.2656\n",
      "Correct positives: 9892\n",
      "All pred positives: 10357\n",
      "All true positives: 11094\n",
      "Accuracy: 0.9601\n",
      "Precision: 0.9551\n",
      "Recall: 0.8917\n",
      "F1: 0.9223\n"
     ]
    }
   ],
   "source": [
    "eval_results(clf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1667\n"
     ]
    }
   ],
   "source": [
    "# Error analysis\n",
    "err_output_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/archive_results/B-token_mentioning_schema-errors-{MODEL_VER}.txt\"\n",
    "\n",
    "test_error_analysis_utter(clf, test_X, test_y, err_output_path=err_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C - schema directly mentioned (schema-only)\n",
    "- check lemma match with tokens\n",
    "- Result (2.12.3.3):\n",
    "    - pos% = 0.1158\n",
    "    - F1 = 0.2677\n",
    "- Result (2.18.2.2):\n",
    "    - pos% = 0.1113\n",
    "    - Train F1 = 0.3463\n",
    "    - F1 = 0.2369\n",
    "- Result (2.23.1.4) (added head):\n",
    "    - pos% = 0.1113\n",
    "    - Train F1 = 0.8145\n",
    "    - F1 = 0.7462"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d322fdafc3342c297bbb2ec53ac77b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5aeee447a274581b506838b2e62d76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather data (X for schema toks, y for task-C)\n",
    "\n",
    "train_samples_C = []\n",
    "test_samples_C = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _schema_len = _metadata['schema_len']\n",
    "        _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _schema_encodings = _token_encodings[_text_len + 1 : _text_len + 1 + _schema_len]\n",
    "        \n",
    "        _labels = []\n",
    "        \n",
    "        _utter_tokens_stem = [stemmer.stem(_t) for _t in _metadata['text_tokens']]\n",
    "        _schema_tokens_stem = [stemmer.stem(_t) for _t in _metadata['schema_tokens']]\n",
    "        for _ut in _schema_tokens_stem:\n",
    "            if _ut in string.punctuation:\n",
    "                _labels.append(0)\n",
    "            elif _ut in _utter_tokens_stem:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "        train_samples_C.extend(list(zip(_schema_encodings, _labels)))\n",
    "\n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _schema_len = _metadata['schema_len']\n",
    "        _token_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _schema_encodings = _token_encodings[_text_len + 1 : _text_len + 1 + _schema_len]\n",
    "        \n",
    "        _labels = []\n",
    "        \n",
    "        _utter_tokens_stem = [stemmer.stem(_t) for _t in _metadata['text_tokens']]\n",
    "        _schema_tokens_stem = [stemmer.stem(_t) for _t in _metadata['schema_tokens']]\n",
    "        for _ut in _schema_tokens_stem:\n",
    "            if _ut in string.punctuation:\n",
    "                _labels.append(0)\n",
    "            elif _ut in _utter_tokens_stem:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "        test_samples_C.extend(list(zip(_schema_encodings, _labels)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593194, 187337)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples_C), len(test_samples_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "train_X = [s[0] for s in train_samples_C]\n",
    "train_y = [s[1] for s in train_samples_C]\n",
    "test_X = [s[0] for s in test_samples_C]\n",
    "test_y = [s[1] for s in test_samples_C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.pyenv/versions/3.7.5/envs/py3.7-speakql/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train --\n",
      "Positive prop: 0.1113\n",
      "Correct positives: 51256\n",
      "All pred positives: 59818\n",
      "All true positives: 66041\n",
      "Accuracy: 0.9606\n",
      "Precision: 0.8569\n",
      "Recall: 0.7761\n",
      "F1: 0.8145\n",
      "-- Test --\n",
      "Positive prop: 0.1342\n",
      "Correct positives: 19635\n",
      "All pred positives: 27490\n",
      "All true positives: 25138\n",
      "Accuracy: 0.9287\n",
      "Precision: 0.7143\n",
      "Recall: 0.7811\n",
      "F1: 0.7462\n"
     ]
    }
   ],
   "source": [
    "print('-- Train --')\n",
    "eval_results(clf, train_X, train_y)\n",
    "print('-- Test --')\n",
    "eval_results(clf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13358\n"
     ]
    }
   ],
   "source": [
    "# Error analysis\n",
    "err_output_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/archive_results/C-schema_directly_mentioned-errors-{MODEL_VER}.txt\"\n",
    "\n",
    "test_error_analysis_schema(clf, test_X, test_y, err_output_path=err_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D - schema implicitly mentioned (schema-only)\n",
    "- check gold SQL\n",
    "- Result (2.12.3.3):\n",
    "    - pos% = 0.0740\n",
    "    - F1 = 0.0321\n",
    "- Result (2.18.2.2):\n",
    "    - pos% = 0.0713\n",
    "    - Train F1 = 0.0848\n",
    "    - F1 = 0.0916\n",
    "- Result (2.23.2.2) (added head):\n",
    "    - pos% = 0.0713\n",
    "    - Train F1 = 0.5722\n",
    "    - F1 = 0.4628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd529457935e49bba887337f2d0cfb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dd591d6b544820afe48bbc2f4eb331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather data (X for schema toks (can use C), y for task-D)\n",
    "\n",
    "# samples_D = []\n",
    "train_labels_D = []\n",
    "test_labels_D = []\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "\n",
    "        schema_id2names = defaultdict(str)\n",
    "        _tmp_ids = []\n",
    "        _tmp_toks = []\n",
    "        for i, _tok in enumerate(_metadata['schema_tokens']):\n",
    "            if _tok in ',.:':\n",
    "                # end of name \n",
    "                _name = '_'.join(_tmp_toks)\n",
    "                for _idx in _tmp_ids:\n",
    "                    schema_id2names[_idx] = _name\n",
    "                \n",
    "                _tmp_ids = []\n",
    "                _tmp_toks = []\n",
    "            else:\n",
    "                _tmp_ids.append(i)\n",
    "                _tmp_toks.append(_tok)\n",
    "                \n",
    "        # assert _tmp_ids == _tmp_toks == []    # Might not be true due to truncating \n",
    "        \n",
    "        sql_schema_names = []\n",
    "        for i, q_tok in enumerate(_cand['query_toks']):\n",
    "            _toks = re.split(r'^[Tt]\\d+\\.', q_tok)\n",
    "            if len(_toks) > 1:\n",
    "                assert len(_toks) == 2 and _toks[0] == '', q_tok\n",
    "            for _tok in _toks:\n",
    "                if _tok.isupper() or _tok in string.punctuation:\n",
    "                    continue\n",
    "                assert ' ' not in _tok, q_tok\n",
    "                sql_schema_names.append(_tok.lower()) # can have '_' in name \n",
    "        \n",
    "        _labels = []\n",
    "        for i in range(len(_metadata['schema_tokens'])):\n",
    "            if schema_id2names[i] in sql_schema_names:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "#         samples_D.extend(list(zip(_schema_encodings, _labels)))\n",
    "        train_labels_D.extend(_labels)\n",
    "\n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "\n",
    "        schema_id2names = defaultdict(str)\n",
    "        _tmp_ids = []\n",
    "        _tmp_toks = []\n",
    "        for i, _tok in enumerate(_metadata['schema_tokens']):\n",
    "            if _tok in ',.:':\n",
    "                # end of name \n",
    "                _name = '_'.join(_tmp_toks)\n",
    "                for _idx in _tmp_ids:\n",
    "                    schema_id2names[_idx] = _name\n",
    "                \n",
    "                _tmp_ids = []\n",
    "                _tmp_toks = []\n",
    "            else:\n",
    "                _tmp_ids.append(i)\n",
    "                _tmp_toks.append(_tok)\n",
    "\n",
    "        # assert _tmp_ids == _tmp_toks == []    # Might not be true due to truncating \n",
    "        \n",
    "        sql_schema_names = []\n",
    "        for i, q_tok in enumerate(_cand['query_toks']):\n",
    "            _toks = re.split(r'^[Tt]\\d+\\.', q_tok)\n",
    "            if len(_toks) > 1:\n",
    "                assert len(_toks) == 2 and _toks[0] == '', q_tok\n",
    "            for _tok in _toks:\n",
    "                if _tok.isupper() or _tok in string.punctuation:\n",
    "                    continue\n",
    "                assert ' ' not in _tok, q_tok\n",
    "                sql_schema_names.append(_tok.lower()) # can have '_' in name \n",
    "        \n",
    "        _labels = []\n",
    "        for i in range(len(_metadata['schema_tokens'])):\n",
    "            if schema_id2names[i] in sql_schema_names:\n",
    "                _labels.append(1)\n",
    "            else:\n",
    "                _labels.append(0)\n",
    "        \n",
    "#         samples_D.extend(list(zip(_schema_encodings, _labels)))\n",
    "        test_labels_D.extend(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _cand['query_toks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593194, 187337)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_D), len(test_labels_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "train_X = [s[0] for s in train_samples_C]\n",
    "train_y = train_labels_D\n",
    "test_X = [s[0] for s in test_samples_C]\n",
    "test_y = test_labels_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/.pyenv/versions/3.7.5/envs/py3.7-speakql/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Train --\n",
      "Positive prop: 0.0713\n",
      "Correct positives: 19621\n",
      "All pred positives: 26274\n",
      "All true positives: 42311\n",
      "Accuracy: 0.9505\n",
      "Precision: 0.7468\n",
      "Recall: 0.4637\n",
      "F1: 0.5722\n",
      "-- Test --\n",
      "Positive prop: 0.0888\n",
      "Correct positives: 7344\n",
      "All pred positives: 15109\n",
      "All true positives: 16630\n",
      "Accuracy: 0.9090\n",
      "Precision: 0.4861\n",
      "Recall: 0.4416\n",
      "F1: 0.4628\n"
     ]
    }
   ],
   "source": [
    "print('-- Train --')\n",
    "eval_results(clf, train_X, train_y)\n",
    "print('-- Test --')\n",
    "eval_results(clf, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17051\n"
     ]
    }
   ],
   "source": [
    "# Error analysis\n",
    "err_output_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/archive_results/D-schema_indirectly_mentioned-errors-{MODEL_VER}.txt\"\n",
    "\n",
    "test_error_analysis_schema(clf, test_X, test_y, err_output_path=err_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Old\n",
    "\n",
    "# corr_pos = 0\n",
    "# all_pred_pos = 0\n",
    "# all_true_pos = 0\n",
    "# for _pred, _y in zip(preds, y):\n",
    "#     corr_pos += _pred * _y\n",
    "#     all_pred_pos += _pred\n",
    "#     all_true_pos += _y\n",
    "\n",
    "# prec = corr_pos / all_pred_pos\n",
    "# rec = corr_pos / all_true_pos\n",
    "# f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "\n",
    "# print(f'Correct positives: {corr_pos}')\n",
    "# print(f'All pred positives: {all_pred_pos}')\n",
    "# print(f'All true positives: {all_true_pos}')\n",
    "# print(f'Precision: {prec:.4f}')\n",
    "# print(f'Recall: {rec:.4f}')\n",
    "# print(f'F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task E - phoneme existance (multilabel) (utterence-only)\n",
    "- Result (2.12.3.1):\n",
    "    - Pos% (averaged over phonemes with pos_prop > 0.05, same below): 0.1011\n",
    "    - Train F1: 0.0193\n",
    "    - Test F1: 0.0221\n",
    "    - For most phonemes, F1 = 0 (even with pos_prop > 0.1, such as N, S, AH0)\n",
    "- Result (2.18.2.2):\n",
    "    - Pos%: 0.0985\n",
    "    - Train F1: 0.2282\n",
    "    - Test F1: 0.2113\n",
    "    - Samples are different because dataset was reloaded; not a big concern, because sample size is large, and test set is identical. If having time, can rerun 2.12.3.1 with the given fixed seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, ['@@UNKNOWN@@', 'sil', 'T', 'D', 'N', 'AH0', 'S', 'R', 'K', 'M'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PHONEME_VOCAB_NAMESPACE = \"phonemes\"\n",
    "phonemes_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/runs/2.18.2.0i/vocabulary/phonemes.txt\"\n",
    "with open(phonemes_path, 'r') as f:\n",
    "    phonemes = f.read().strip().split('\\n')\n",
    "len(phonemes), phonemes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ce201f763443f79015cfda278c7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d0f4a0c6364297913cb04de7e432b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather data (X audio for utter toks, y for task-E per phoneme)\n",
    "\n",
    "train_feats_E = []\n",
    "test_feats_E = []\n",
    "train_labels_E_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "test_labels_E_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _audio_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['audio_feats_encoded'][0]\n",
    "#         _audio_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _utter_audio_encodings = _audio_encodings[:_text_len]\n",
    "        \n",
    "        train_feats_E.extend(_utter_audio_encodings)\n",
    "        \n",
    "        assert len(_cand['token_phonemes']) == len(_utter_audio_encodings) == _text_len, \\\n",
    "            (_cand['token_phonemes'], len(_utter_audio_encodings), _text_len)\n",
    "        for _tok_phs in _cand['token_phonemes']:\n",
    "            _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "            \n",
    "            if _tok_phs is not None:\n",
    "                for _ph in _tok_phs:\n",
    "                    if _ph not in phonemes:\n",
    "                        _ph_exists['@@UNKNOWN@@'] = True\n",
    "                    else:\n",
    "                        _ph_exists[_ph] = True\n",
    "            # if _tok_phs is None, empty set as default \n",
    "            for _ph in phonemes:\n",
    "                train_labels_E_per_ph[_ph].append(int(_ph_exists[_ph]))\n",
    "    \n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _text_len = _metadata['text_len']\n",
    "        _audio_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['audio_feats_encoded'][0]\n",
    "#         _audio_encodings = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "        _utter_audio_encodings = _audio_encodings[:_text_len]\n",
    "        \n",
    "        test_feats_E.extend(_utter_audio_encodings)\n",
    "        \n",
    "        assert len(_cand['token_phonemes']) == len(_utter_audio_encodings) == _text_len, \\\n",
    "            (_cand['token_phonemes'], len(_utter_audio_encodings), _text_len)\n",
    "        for _tok_phs in _cand['token_phonemes']:\n",
    "            _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "            \n",
    "            if _tok_phs is not None:\n",
    "                for _ph in _tok_phs:\n",
    "                    if _ph not in phonemes:\n",
    "                        _ph_exists['@@UNKNOWN@@'] = True\n",
    "                    else:\n",
    "                        _ph_exists[_ph] = True\n",
    "            # if _tok_phs is None, empty set as default \n",
    "            for _ph in phonemes:\n",
    "                test_labels_E_per_ph[_ph].append(int(_ph_exists[_ph]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85292, 41776)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_feats_E), len(test_feats_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_res_dicts = dict()  # Dict[str:ph, Dict:results]\n",
    "test_res_dicts = dict()\n",
    "\n",
    "for _ph in tqdm(phonemes):\n",
    "    if _ph in {'@@UNKNOWN@@', 'sil', '[NONE]', ''}:\n",
    "        # not an actual phoneme \n",
    "        continue\n",
    "    \n",
    "    print(f'Phoneme: {_ph}')\n",
    "    clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "    train_X = train_feats_E\n",
    "    train_y = train_labels_E_per_ph[_ph]\n",
    "    test_X = test_feats_E\n",
    "    test_y = test_labels_E_per_ph[_ph]\n",
    "    \n",
    "    if sum(train_y) == 0 or sum(test_y) == 0:\n",
    "        # phoneme not exist in dataset \n",
    "        print(f'{_ph} not in dataset')\n",
    "        print()\n",
    "        continue\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    _train_d = dict()\n",
    "    _test_d = dict()\n",
    "    eval_results(clf, train_X, train_y, res_dict=_train_d)\n",
    "    eval_results(clf, test_X, test_y, res_dict=_test_d)\n",
    "    train_res_dicts[_ph] = _train_d\n",
    "    test_res_dicts[_ph] = _test_d\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09845840507567281\n",
      "0.2281778533661335\n",
      "0.21130471085969618\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([train_res_dicts[_ph]['pos_prop'] for _ph in train_res_dicts if train_res_dicts[_ph]['pos_prop'] > 0.05]))\n",
    "print(np.mean([train_res_dicts[_ph]['f1'] for _ph in train_res_dicts if train_res_dicts[_ph]['pos_prop'] > 0.05]))\n",
    "print(np.mean([test_res_dicts[_ph]['f1'] for _ph in train_res_dicts if train_res_dicts[_ph]['pos_prop'] > 0.05]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T', 0.20149603714299114, 0.37116764472342895),\n",
       " ('D', 0.11371523706795479, 0.14226489184585123),\n",
       " ('N', 0.1979435351498382, 0.24902939514159889),\n",
       " ('AH0', 0.14976785630539793, 0.07930525952786863),\n",
       " ('S', 0.14304975847676218, 0.34475045388698267),\n",
       " ('R', 0.12463068048586033, 0.3836438265586174),\n",
       " ('K', 0.08960981100220419, 0.2250584978536747),\n",
       " ('M', 0.12391549031562163, 0.16122531215621105),\n",
       " ('L', 0.10229564320217606, 0.444889110780385),\n",
       " ('P', 0.057062796041832765, 0.05973025034868963),\n",
       " ('EY1', 0.06892791821038316, 0.1813530411299053),\n",
       " ('IH0', 0.09059466303990996, 0.0),\n",
       " ('ER0', 0.0821179008582282, 0.1391527597244173),\n",
       " ('Z', 0.11481733339586363, 0.30796938271590174),\n",
       " ('AE1', 0.06946724194531727, 0.0),\n",
       " ('IH1', 0.0559138019978427, 0.14818355622460053),\n",
       " ('AH1', 0.06053322703184355, 0.37074100509132196),\n",
       " ('IY0', 0.05875111382075693, 0.20148588374510407),\n",
       " ('EH1', 0.05414341321577639, 0.0),\n",
       " ('F', 0.05348684519063922, 0.2003159555782784),\n",
       " ('W', 0.055386202691928905, 0.427132701020782)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_included_phs = [_ph for _ph in train_res_dicts if train_res_dicts[_ph]['pos_prop'] > 0.05]\n",
    "[(_ph, train_res_dicts[_ph]['pos_prop'], test_res_dicts[_ph]['f1']) for _ph in _included_phs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sum(_f) for _f in train_feats_E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug checking\n",
    "o_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset[6712][0]['token_phonemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_E_per_ph['D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_feats_E[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_log_proba([train_feats_E[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=0.0001, C=100.0, class_weight='balanced')\n",
    "\n",
    "train_X = train_feats_E\n",
    "train_y = train_labels_E_per_ph['T']\n",
    "test_X = test_feats_E\n",
    "test_y = test_labels_E_per_ph['T']\n",
    "\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results(clf, train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task R1 - decoder awareness of corresponding input text phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, ['@@UNKNOWN@@', 'T', 'AH', 'D', 'N', 'IH', 'S', '[NONE]', 'R', 'K'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PHONEME_VOCAB_NAMESPACE = \"phonemes\"\n",
    "phonemes_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/runs/2.29.0.0i/vocabulary/phonemes.txt\"\n",
    "with open(phonemes_path, 'r') as f:\n",
    "    phonemes = f.read().strip().split('\\n')\n",
    "len(phonemes), phonemes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _decoder_intermediates[f'state:step_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unifying train & test loading as a function, to avoid copying back and forth \n",
    "## TODO: add 'phonemes' as argument \n",
    "def Load_probing_data_R1(predictor, ds_instances, ds_original, dataset_reader):\n",
    "    '''\n",
    "    example:\n",
    "    Load_probing_data_R1(predictor, train_dataset, full_train_dataset, train_dataset_reader)\n",
    "    Load_probing_data_R1(predictor, test_dataset, full_dev_dataset, test_dataset_reader)\n",
    "    '''\n",
    "    \n",
    "    feats_R1 = []\n",
    "    labels_R1_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "    # debug checking \n",
    "    tokens_R1 = []\n",
    "    \n",
    "    # Gather instances by o_id \n",
    "    o_id2instances = defaultdict(list)\n",
    "    for _inst in ds_instances:\n",
    "        o_id = _inst['metadata']['original_id']\n",
    "        o_id2instances[o_id].append(_inst)\n",
    "        \n",
    "    for o_id, _insts in tqdm(o_id2instances.items(), total=len(o_id2instances)):\n",
    "        _cands = ds_original[o_id]\n",
    "\n",
    "        for _inst, _cand in zip(_insts, _cands):\n",
    "            _pred = predictor.predict_instance(_inst)\n",
    "\n",
    "            _metadata = _inst['metadata']\n",
    "            _target_toks = [str(t) for t in _metadata['target_tokens']] + [END_SYMBOL]\n",
    "            _decoder_intermediates = _pred['rewrite_seq_prediction_intermediates']['decoder']\n",
    "            # len(_decoder_hidden_states) == len(_target_toks) from metadata; including the last step predicting @end@ \n",
    "            # [0] to get rid of batch dim \n",
    "            _decoder_hidden_states = [_decoder_intermediates[f'state_decoder_hidden:step_{i}'][0] for i in range(len(_target_toks))]\n",
    "\n",
    "            feats_R1.extend(_decoder_hidden_states)\n",
    "            tokens_R1.extend(_target_toks)\n",
    "\n",
    "            # List[List[int]], the ids of each span in target sequence \n",
    "            _target_spans = []\n",
    "            _curr_span = []\n",
    "            for i, t in enumerate(_target_toks):\n",
    "                _curr_span.append(i)\n",
    "                if t in {'[ANS]', END_SYMBOL}:\n",
    "                    _target_spans.append(_curr_span)\n",
    "                    _curr_span = []\n",
    "\n",
    "            assert len(_curr_span) == 0, _target_toks\n",
    "            ## _target_spans include trailing END_SYMBOL as a span, so 1 more \n",
    "            assert len(_target_spans) == len(_cand['rewriter_edits']) + 1, f\"{_target_toks} || {_cand['rewriter_edits']}\"\n",
    "\n",
    "            for e_i, edit_d in enumerate(_cand['rewriter_edits']):\n",
    "                q_span = ' '.join([_cand['question_toks'][i].lower() for i in edit_d['src_span']])\n",
    "                q_span_phs = [ph for q_tok in text_cell_to_toks(q_span) for ph in dataset_reader._token_to_phonemes(q_tok)]\n",
    "\n",
    "                _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "                for _ph in q_span_phs:\n",
    "                    if _ph not in phonemes:\n",
    "                        _ph_exists['@@UNKNOWN@@'] = True\n",
    "                    else:\n",
    "                        _ph_exists[_ph] = True\n",
    "\n",
    "                for _ph in phonemes:\n",
    "                    # for each target step in the target span correspond to this src span, add these ph labels \n",
    "                    labels_R1_per_ph[_ph].extend([int(_ph_exists[_ph])] * len(_target_spans[e_i]))\n",
    "            # for the last step END_SYMBOL span\n",
    "            for _ph in phonemes:\n",
    "                labels_R1_per_ph[_ph].extend([0])\n",
    "\n",
    "    return {\n",
    "        \"feats_R1\": feats_R1,\n",
    "        \"labels_R1_per_ph\": labels_R1_per_ph,\n",
    "        \"tokens_R1\": tokens_R1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211535da82b146dc91a23a01ae37224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_probing_data_R1 = Load_probing_data_R1(predictor, train_dataset, full_train_dataset, train_dataset_reader)\n",
    "# train_probing_data_R1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b91b6aec874787ad70206fca9bd38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_probing_data_R1 = Load_probing_data_R1(predictor, test_dataset, full_dev_dataset, test_dataset_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_R1 = train_probing_data_R1['feats_R1']\n",
    "train_tokens_R1 = train_probing_data_R1['tokens_R1']\n",
    "train_labels_R1_per_ph = train_probing_data_R1['labels_R1_per_ph']\n",
    "test_feats_R1 = test_probing_data_R1['feats_R1']\n",
    "test_tokens_R1 = test_probing_data_R1['tokens_R1']\n",
    "test_labels_R1_per_ph = test_probing_data_R1['labels_R1_per_ph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27629, 256), 27629, 27629, (4469, 256), 4469, 4469)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_feats_R1).shape, len(train_tokens_R1), len(train_labels_R1_per_ph['S']), \\\n",
    "np.array(test_feats_R1).shape, len(test_tokens_R1), len(test_labels_R1_per_ph['S'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression probes \n",
    "\n",
    "train_res_dicts = dict()  # Dict[str:ph, Dict:results]\n",
    "test_res_dicts = dict()\n",
    "\n",
    "for _ph in tqdm(phonemes):\n",
    "    if _ph in {'@@UNKNOWN@@', 'sil', '[NONE]', ''}:\n",
    "        # not an actual phoneme \n",
    "        continue\n",
    "    \n",
    "    print(f'Phoneme: {_ph}')\n",
    "    clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "    train_X = train_feats_R1\n",
    "    train_y = train_labels_R1_per_ph[_ph]\n",
    "    test_X = test_feats_R1\n",
    "    test_y = test_labels_R1_per_ph[_ph]\n",
    "    \n",
    "    if sum(train_y) == 0 or sum(test_y) == 0:\n",
    "        # phoneme not exist in dataset \n",
    "        print(f'{_ph} not in dataset')\n",
    "        print()\n",
    "        continue\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    _train_d = dict()\n",
    "    _test_d = dict()\n",
    "    eval_results(clf, train_X, train_y, res_dict=_train_d)\n",
    "    eval_results(clf, test_X, test_y, res_dict=_test_d)\n",
    "    train_res_dicts[_ph] = _train_d\n",
    "    test_res_dicts[_ph] = _test_d\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save probing results \n",
    "\n",
    "analysis_dir = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/analysis_results'\n",
    "train_save_path = os.path.join(analysis_dir, 'probing_R1_results_train.csv')\n",
    "test_save_path = os.path.join(analysis_dir, 'probing_R1_results_test.csv')\n",
    "pd.DataFrame.from_dict(train_res_dicts, orient='index').to_csv(train_save_path)\n",
    "pd.DataFrame.from_dict(test_res_dicts, orient='index').to_csv(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_probing_data_R1\n",
    "# del test_probing_data_R1\n",
    "# del train_feats_R1\n",
    "# del train_tokens_R1\n",
    "# del train_labels_R1_per_ph\n",
    "# del test_feats_R1\n",
    "# del test_tokens_R1\n",
    "# del test_labels_R1_per_ph\n",
    "\n",
    "## Using del seems useless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task R1a - compare of R1: probing the phoneme tag embeddings\n",
    "- Should have ~100% accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unifying train & test loading as a function, to avoid copying back and forth \n",
    "## TODO: add 'phonemes' as argument \n",
    "## R1_compare_a: use phoneme embeddings a token to probe its phonemes on token level \n",
    "## tokens still from edit spans \n",
    "def Load_probing_data_R1_compare_a(predictor, ds_instances, ds_original, dataset_reader):\n",
    "    feats_R1 = []\n",
    "    labels_R1_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "    # debug checking \n",
    "    tokens_R1 = []\n",
    "    \n",
    "    # Gather instances by o_id \n",
    "    o_id2instances = defaultdict(list)\n",
    "    for _inst in ds_instances:\n",
    "        o_id = _inst['metadata']['original_id']\n",
    "        o_id2instances[o_id].append(_inst)\n",
    "        \n",
    "    for o_id, _insts in tqdm(o_id2instances.items(), total=len(o_id2instances)):\n",
    "        _cands = ds_original[o_id]\n",
    "\n",
    "        for _inst, _cand in zip(_insts, _cands):\n",
    "            _pred = predictor.predict_instance(_inst)\n",
    "\n",
    "            _metadata = _inst['metadata']\n",
    "            _source_toks = [str(t) for t in _metadata['source_tokens']]  # concat tokens, but text in front \n",
    "            _intermediates = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "\n",
    "            for e_i, edit_d in enumerate(_cand['rewriter_edits']):\n",
    "                for i in edit_d['src_span']:\n",
    "                    feats_R1.append(_intermediates[i])\n",
    "                    tokens_R1.append(_source_toks[i])\n",
    "\n",
    "                    _tok = _source_toks[i]\n",
    "                    _tok_phs = [ph for _t in text_cell_to_toks(_tok) for ph in dataset_reader._token_to_phonemes(_t)]\n",
    "\n",
    "                    _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "                    for _ph in _tok_phs:\n",
    "                        if _ph not in phonemes:\n",
    "                            _ph_exists['@@UNKNOWN@@'] = True\n",
    "                        else:\n",
    "                            _ph_exists[_ph] = True\n",
    "\n",
    "                    for _ph in phonemes:\n",
    "                        # for each target step in the target span correspond to this src span, add these ph labels \n",
    "                        labels_R1_per_ph[_ph].append(int(_ph_exists[_ph]))\n",
    "\n",
    "    return {\n",
    "        \"feats_R1a\": feats_R1,\n",
    "        \"labels_R1a_per_ph\": labels_R1_per_ph,\n",
    "        \"tokens_R1a\": tokens_R1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0098893ad4c4cd38688e72d8980f9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_probing_data_R1a = Load_probing_data_R1_compare_a(predictor, train_dataset, full_train_dataset, train_dataset_reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0249d2d512de49578dc87586330294e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_probing_data_R1a = Load_probing_data_R1_compare_a(predictor, test_dataset, full_dev_dataset, test_dataset_reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_R1a = train_probing_data_R1a['feats_R1a']\n",
    "train_tokens_R1a = train_probing_data_R1a['tokens_R1a']\n",
    "train_labels_R1a_per_ph = train_probing_data_R1a['labels_R1a_per_ph']\n",
    "test_feats_R1a = test_probing_data_R1a['feats_R1a']\n",
    "test_tokens_R1a = test_probing_data_R1a['tokens_R1a']\n",
    "test_labels_R1a_per_ph = test_probing_data_R1a['labels_R1a_per_ph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10661, 256), 10661, 10661, (1744, 256), 1744, 1744)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_feats_R1a).shape, len(train_tokens_R1a), len(train_labels_R1a_per_ph['S']), \\\n",
    "np.array(test_feats_R1a).shape, len(test_tokens_R1a), len(test_labels_R1a_per_ph['S'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression probes \n",
    "\n",
    "train_res_dicts = dict()  # Dict[str:ph, Dict:results]\n",
    "test_res_dicts = dict()\n",
    "\n",
    "for _ph in tqdm(phonemes):\n",
    "    if _ph in {'@@UNKNOWN@@', 'sil', '[NONE]', ''}:\n",
    "        # not an actual phoneme \n",
    "        continue\n",
    "    \n",
    "    print(f'Phoneme: {_ph}')\n",
    "    clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "    train_X = train_feats_R1a\n",
    "    train_y = train_labels_R1a_per_ph[_ph]\n",
    "    test_X = test_feats_R1a\n",
    "    test_y = test_labels_R1a_per_ph[_ph]\n",
    "    \n",
    "    if sum(train_y) == 0 or sum(test_y) == 0:\n",
    "        # phoneme not exist in dataset \n",
    "        print(f'{_ph} not in dataset')\n",
    "        print()\n",
    "        continue\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    _train_d = dict()\n",
    "    _test_d = dict()\n",
    "    eval_results(clf, train_X, train_y, res_dict=_train_d)\n",
    "    eval_results(clf, test_X, test_y, res_dict=_test_d)\n",
    "    train_res_dicts[_ph] = _train_d\n",
    "    test_res_dicts[_ph] = _test_d\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save probing results \n",
    "\n",
    "analysis_dir = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/analysis_results'\n",
    "train_save_path = os.path.join(analysis_dir, 'probing_R1a_results_train.csv')\n",
    "test_save_path = os.path.join(analysis_dir, 'probing_R1a_results_test.csv')\n",
    "pd.DataFrame.from_dict(train_res_dicts, orient='index').to_csv(train_save_path)\n",
    "pd.DataFrame.from_dict(test_res_dicts, orient='index').to_csv(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task R1b - compare of R1: probing the encoder representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unifying train & test loading as a function, to avoid copying back and forth \n",
    "## TODO: add 'phonemes' as argument \n",
    "## R1_compare_b: use encoder representation of a token to probe its edit span phonemes \n",
    "def Load_probing_data_R1_compare_b(predictor, ds_instances, ds_original, dataset_reader):\n",
    "    \n",
    "    feats_R1 = []\n",
    "    labels_R1_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "    # debug checking \n",
    "    tokens_R1 = []\n",
    "    \n",
    "    # Gather instances by o_id \n",
    "    o_id2instances = defaultdict(list)\n",
    "    for _inst in ds_instances:\n",
    "        o_id = _inst['metadata']['original_id']\n",
    "        o_id2instances[o_id].append(_inst)\n",
    "        \n",
    "    for o_id, _insts in tqdm(o_id2instances.items(), total=len(o_id2instances)):\n",
    "        _cands = ds_original[o_id]\n",
    "\n",
    "        for _inst, _cand in zip(_insts, _cands):\n",
    "            _pred = predictor.predict_instance(_inst)\n",
    "\n",
    "            _metadata = _inst['metadata']\n",
    "            _source_toks = [str(t) for t in _metadata['source_tokens']] + [END_SYMBOL]\n",
    "            _intermediates = _pred['rewrite_seq_prediction_intermediates']['rewriter_main']['encoder_seq_representation_with_tag'][0]\n",
    "#             feats_R1.extend(_intermediates)\n",
    "#             tokens_R1.extend(_source_toks)\n",
    "\n",
    "            for e_i, edit_d in enumerate(_cand['rewriter_edits']):\n",
    "                q_span = ' '.join([_cand['question_toks'][i].lower() for i in edit_d['src_span']])\n",
    "                q_span_phs = [ph for q_tok in text_cell_to_toks(q_span) for ph in dataset_reader._token_to_phonemes(q_tok)]\n",
    "\n",
    "                feats_R1.extend([_intermediates[i] for i in edit_d['src_span']])\n",
    "                tokens_R1.extend([_source_toks[i] for i in edit_d['src_span']])\n",
    "                \n",
    "                _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "                for _ph in q_span_phs:\n",
    "                    if _ph not in phonemes:\n",
    "                        _ph_exists['@@UNKNOWN@@'] = True\n",
    "                    else:\n",
    "                        _ph_exists[_ph] = True\n",
    "\n",
    "                for _ph in phonemes:\n",
    "                    # for each target step in the target span correspond to this src span, add these ph labels \n",
    "                    labels_R1_per_ph[_ph].extend([int(_ph_exists[_ph])] * len(edit_d['src_span']))\n",
    "\n",
    "    return {\n",
    "        \"feats_R1b\": feats_R1,\n",
    "        \"labels_R1b_per_ph\": labels_R1_per_ph,\n",
    "        \"tokens_R1b\": tokens_R1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ff13e7c04c483b80d17313f0c7e1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_probing_data_R1b = Load_probing_data_R1_compare_b(predictor, train_dataset, full_train_dataset, train_dataset_reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3dfe9fa3f341c1acc8a3332395ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_probing_data_R1b = Load_probing_data_R1_compare_b(predictor, test_dataset, full_dev_dataset, test_dataset_reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_R1b = train_probing_data_R1b['feats_R1b']\n",
    "train_tokens_R1b = train_probing_data_R1b['tokens_R1b']\n",
    "train_labels_R1b_per_ph = train_probing_data_R1b['labels_R1b_per_ph']\n",
    "test_feats_R1b = test_probing_data_R1b['feats_R1b']\n",
    "test_tokens_R1b = test_probing_data_R1b['tokens_R1b']\n",
    "test_labels_R1b_per_ph = test_probing_data_R1b['labels_R1b_per_ph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10661, 256), 10661, 10661, (1744, 256), 1744, 1744)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_feats_R1b).shape, len(train_tokens_R1b), len(train_labels_R1b_per_ph['S']), \\\n",
    "np.array(test_feats_R1b).shape, len(test_tokens_R1b), len(test_labels_R1b_per_ph['S'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression probes \n",
    "\n",
    "train_res_dicts = dict()  # Dict[str:ph, Dict:results]\n",
    "test_res_dicts = dict()\n",
    "\n",
    "for _ph in tqdm(phonemes):\n",
    "    if _ph in {'@@UNKNOWN@@', 'sil', '[NONE]', ''}:\n",
    "        # not an actual phoneme \n",
    "        continue\n",
    "    \n",
    "    print(f'Phoneme: {_ph}')\n",
    "    clf = LogisticRegression(tol=0.0001, C=100.0)\n",
    "\n",
    "    train_X = train_feats_R1b\n",
    "    train_y = train_labels_R1b_per_ph[_ph]\n",
    "    test_X = test_feats_R1b\n",
    "    test_y = test_labels_R1b_per_ph[_ph]\n",
    "    \n",
    "    if sum(train_y) == 0 or sum(test_y) == 0:\n",
    "        # phoneme not exist in dataset \n",
    "        print(f'{_ph} not in dataset')\n",
    "        print()\n",
    "        continue\n",
    "    \n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    _train_d = dict()\n",
    "    _test_d = dict()\n",
    "    eval_results(clf, train_X, train_y, res_dict=_train_d)\n",
    "    eval_results(clf, test_X, test_y, res_dict=_test_d)\n",
    "    train_res_dicts[_ph] = _train_d\n",
    "    test_res_dicts[_ph] = _test_d\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save probing results \n",
    "\n",
    "analysis_dir = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/analysis_results'\n",
    "train_save_path = os.path.join(analysis_dir, 'probing_R1b_results_train.csv')\n",
    "test_save_path = os.path.join(analysis_dir, 'probing_R1b_results_test.csv')\n",
    "pd.DataFrame.from_dict(train_res_dicts, orient='index').to_csv(train_save_path)\n",
    "pd.DataFrame.from_dict(test_res_dicts, orient='index').to_csv(test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_sen = 'What is the id of students of grade 5 .'\n",
    "[_t.lemma_ for _t in nlp(_sen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "[stemmer.stem(_t) for _t in _sen.split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legacy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39858ce5a3e44e6794752dfead102d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gather data (X decoder hidden states, y corresponding source span text)\n",
    "\n",
    "train_feats_R1 = []\n",
    "train_labels_R1_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "\n",
    "# debug checking \n",
    "train_tokens_R1 = []\n",
    "\n",
    "for o_id, _insts in tqdm(train_o_id2instances.items(), total=len(train_o_id2instances)):\n",
    "    _cands = full_train_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _target_toks = [str(t) for t in _metadata['target_tokens']] + [END_SYMBOL]\n",
    "        _decoder_intermediates = _pred['rewrite_seq_prediction_intermediates']['decoder']\n",
    "        # len(_decoder_hidden_states) == len(_target_toks) from metadata; including the last step predicting @end@ \n",
    "        _decoder_hidden_states = [_decoder_intermediates[f'state_decoder_hidden:step_{i}'] for i in range(len(_target_toks))]\n",
    "        \n",
    "        train_feats_R1.extend(_decoder_hidden_states)\n",
    "        train_tokens_R1.extend(_target_toks)\n",
    "        \n",
    "        # List[List[int]], the ids of each span in target sequence \n",
    "        _target_spans = []\n",
    "        _curr_span = []\n",
    "        for i, t in enumerate(_target_toks):\n",
    "            _curr_span.append(i)\n",
    "            if t in {'[ANS]', END_SYMBOL}:\n",
    "                _target_spans.append(_curr_span)\n",
    "                _curr_span = []\n",
    "                \n",
    "        assert len(_curr_span) == 0, _target_toks\n",
    "        ## _target_spans include trailing END_SYMBOL as a span, so 1 more \n",
    "        assert len(_target_spans) == len(_cand['rewriter_edits']) + 1, f\"{_target_toks} || {_cand['rewriter_edits']}\"\n",
    "        \n",
    "        for e_i, edit_d in enumerate(_cand['rewriter_edits']):\n",
    "            q_span = ' '.join([_cand['question_toks'][i].lower() for i in edit_d['src_span']])\n",
    "            q_span_phs = [ph for q_tok in text_cell_to_toks(q_span) for ph in train_dataset_reader._token_to_phonemes(q_tok)]\n",
    "            \n",
    "            ### TODO: add ph labels correspond to target span \n",
    "            _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "            for _ph in q_span_phs:\n",
    "                if _ph not in phonemes:\n",
    "                    _ph_exists['@@UNKNOWN@@'] = True\n",
    "                else:\n",
    "                    _ph_exists[_ph] = True\n",
    "            \n",
    "            for _ph in phonemes:\n",
    "                # for each target step in the target span correspond to this src span, add these ph labels \n",
    "                train_labels_R1_per_ph[_ph].extend([int(_ph_exists[_ph])] * len(_target_spans[e_i]))\n",
    "        # for the last step END_SYMBOL span\n",
    "        for _ph in phonemes:\n",
    "            train_labels_R1_per_ph[_ph].extend([0])\n",
    "\n",
    "    if len(train_feats_R1) > 10:\n",
    "        break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats_R1 = []\n",
    "test_labels_R1_per_ph = dict([(_ph, []) for _ph in phonemes])\n",
    "\n",
    "test_tokens_R1 = []\n",
    "\n",
    "for o_id, _insts in tqdm(test_o_id2instances.items(), total=len(test_o_id2instances)):\n",
    "    _cands = full_dev_dataset[o_id]\n",
    "    \n",
    "    for _inst, _cand in zip(_insts, _cands):\n",
    "        _pred = predictor.predict_instance(_inst)\n",
    "        \n",
    "        _metadata = _inst['metadata']\n",
    "        _target_toks = [str(t) for t in _metadata['target_tokens']] + [END_SYMBOL]\n",
    "        _decoder_intermediates = _pred['rewrite_seq_prediction_intermediates']['decoder']\n",
    "        # len(_decoder_hidden_states) == len(_target_toks) from metadata; including the last step predicting @end@ \n",
    "        _decoder_hidden_states = [_decoder_intermediates[f'state:step_{i}']['decoder_hidden'] for i in range(len(_target_toks))]\n",
    "        \n",
    "        test_feats_R1.extend(_decoder_hidden_states)\n",
    "        test_tokens_R1.extend(_target_toks)\n",
    "        \n",
    "        # List[List[int]], the ids of each span in target sequence \n",
    "        _target_spans = []\n",
    "        _curr_span = []\n",
    "        for i, t in enumerate(_target_toks):\n",
    "            _curr_span.append(i)\n",
    "            if t in {'[ANS]', END_SYMBOL}:\n",
    "                _target_spans.append(_curr_span)\n",
    "                _curr_span = []\n",
    "        assert len(_curr_span) == 0, _target_toks\n",
    "        assert len(_target_spans) == len(_cand['rewriter_edits']) + 1, f\"{_target_toks} || {_cand['rewriter_edits']}\"\n",
    "        \n",
    "        for e_i, edit_d in enumerate(_cand['rewriter_edits']):\n",
    "            q_span = ' '.join([_cand['question_toks'][i].lower() for i in edit_d['src_span']])\n",
    "            q_span_phs = [ph for q_tok in text_cell_to_toks(q_span) for ph in test_dataset_reader._token_to_phonemes(q_tok)]\n",
    "            \n",
    "            ### TODO: add ph labels correspond to target span \n",
    "            _ph_exists = defaultdict(lambda: False) # Dict[str: ph, bool]\n",
    "            for _ph in q_span_phs:\n",
    "                if _ph not in phonemes:\n",
    "                    _ph_exists['@@UNKNOWN@@'] = True\n",
    "                else:\n",
    "                    _ph_exists[_ph] = True\n",
    "            \n",
    "            for _ph in phonemes:\n",
    "                # for each target step in the target span correspond to this src span, add these ph labels \n",
    "                test_labels_R1_per_ph[_ph].extend([int(_ph_exists[_ph])] * len(_target_spans[e_i]))\n",
    "        # for the last step END_SYMBOL span\n",
    "        for _ph in phonemes:\n",
    "            test_labels_R1_per_ph[_ph].extend([0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
