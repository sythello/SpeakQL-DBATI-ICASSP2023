{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f938f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict, Optional, cast\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss, CosineEmbeddingLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, ArrayField, MetadataField, ListField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding, TokenEmbedder\n",
    "from allennlp.modules.token_embedders.pretrained_transformer_embedder import PretrainedTransformerEmbedder\n",
    "from allennlp.modules.token_embedders.pretrained_transformer_mismatched_embedder import PretrainedTransformerMismatchedEmbedder\n",
    "# from allennlp.modules.seq2seq_encoders.multi_head_self_attention import MultiHeadSelfAttention\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
    "from allennlp.modules.attention import Attention\n",
    "from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n",
    "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
    "from allennlp.modules.matrix_attention.cosine_matrix_attention import CosineMatrixAttention\n",
    "from allennlp.modules.matrix_attention.bilinear_matrix_attention import BilinearMatrixAttention\n",
    "\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions, ConditionalRandomField\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits, \\\n",
    "    get_device_of, masked_softmax, weighted_sum, \\\n",
    "    get_mask_from_sequence_lengths, get_lengths_from_binary_sequence_mask, tensors_equal, \\\n",
    "    batched_span_select\n",
    "\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy, MeanAbsoluteError, Average\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.dataloader import DataLoader, PyTorchDataLoader\n",
    "from allennlp.training.trainer import GradientDescentTrainer\n",
    "# from allennlp.predictors import Predictor, Seq2SeqPredictor, SimpleSeq2SeqPredictor, SentenceTaggerPredictor\n",
    "from allennlp.predictors import Predictor, SentenceTaggerPredictor\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.common.util import JsonDict, sanitize\n",
    "\n",
    "from allennlp_models.generation.predictors import Seq2SeqPredictor\n",
    "from allennlp_models.generation.models.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp_models.generation.modules.seq_decoders.seq_decoder import SeqDecoder\n",
    "from allennlp_models.generation.modules.decoder_nets.decoder_net import DecoderNet\n",
    "\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# from spacy.tokenizer import Tokenizer as SpacyTokenizer\n",
    "# from spacy.lang.en import English\n",
    "# nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "# tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import ShortTermFeatures\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "from inspect import signature\n",
    "import warnings\n",
    "import pickle\n",
    "from copy import copy, deepcopy\n",
    "from overrides import overrides\n",
    "import importlib\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig, BertTokenizer\n",
    "\n",
    "from utils.spider import process_sql, evaluation\n",
    "from utils.schema_gnn.spider_utils import Table, TableColumn, read_dataset_schema\n",
    "from utils.misc_utils import Postprocess_rewrite_seq\n",
    "\n",
    "import dataset_readers, models\n",
    "\n",
    "from dataset_readers.reader_utils import extractAudioFeatures, extractAudioFeatures_NoPooling, dbToTokens, \\\n",
    "    read_DB, Get_align_tags\n",
    "# from dataset_readers.reranker_reader_legacy import SpiderASRRerankerReaderV1, SpiderASRRerankerReaderV2\n",
    "# from dataset_readers.reranker_reader import SpiderASRRerankerReaderV2_Siamese\n",
    "from modules.encoder import SpeakQLEncoder, SpeakQLEncoderV1\n",
    "# from models.reranker import SpiderASRRerankerV0, SpiderASRRerankerV1, SpiderASRRerankerV2, SpiderASRReranker_Siamese\n",
    "# from predictors.reranker_predictor import SpiderASRRerankerPredictor, SpiderASRRerankerPredictor_Siamese\n",
    "\n",
    "# from dataset_readers.rewriter_s2s_tabert_reader import SpiderASRRewriterReader_Seq2seq_TaBERT\n",
    "# from models.rewriter_s2s_tabert import SpiderASRRewriter_Seq2seq_TaBERT \n",
    "# from predictors.rewriter_predictor import SpiderASRRewriterPredictor_Tagger_ILM, SpiderASRRewriterPredictor_Seq2seq\n",
    "from dataset_readers import SpiderASRRewriterReader_ILM_Combined_new, SpiderASRRewriterReader_Seq2seq_Combined_new\n",
    "from models import SpiderASRRewriter_ILM_Combined_new, SpiderASRRewriter_Seq2seq_Combined_new\n",
    "\n",
    "# import dataset_readers.rewriter_reader\n",
    "# import models.rewriter\n",
    "# import predictors.rewriter_predictor\n",
    "\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(dataset_readers.rewriter_s2s_tabert_reader)\n",
    "# importlib.reload(models.rewriter_s2s_tabert)\n",
    "# # importlib.reload(models.rewriter)\n",
    "# # importlib.reload(predictors.rewriter_predictor)\n",
    "\n",
    "# from dataset_readers.rewriter_s2s_tabert_reader import SpiderASRRewriterReader_Seq2seq_TaBERT\n",
    "# from models.rewriter_s2s_tabert import SpiderASRRewriter_Seq2seq_TaBERT \n",
    "# # from models.rewriter import SpiderASRRewriter_Tagger_ILM, SpiderASRRewriter_Seq2seq\n",
    "# # from predictors.rewriter_predictor import SpiderASRRewriterPredictor_Tagger_ILM, SpiderASRRewriterPredictor_Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIM = 136\n",
    "AUDIO_DIM_NO_POOLING = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger-ILM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger-ILM - Dataset Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset_readers import SpiderASRRewriterReader_ILM_Combined\n",
    "# from models import SpiderASRRewriter_ILM_Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa76454409214ec48130b1d1bcfdac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading literals failed: wta_1::players\n",
      "['first_name', 'last_name', 'hand', 'country_code']\n",
      "Could not decode to UTF-8 column 'last_name' with text 'Treyes Albarrac��N'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d7794c60ac454f97182a4bd07eb5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading literals failed: wta_1::players\n",
      "['first_name', 'last_name', 'hand', 'country_code']\n",
      "Could not decode to UTF-8 column 'last_name' with text 'Treyes Albarrac��N'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tables_json_fname = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "dataset_dir = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my'\n",
    "databases_dir = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database'\n",
    "\n",
    "src_token_indexers = {\n",
    "    \"bert\": TokenIndexer.by_name('pretrained_transformer_mismatched')('bert-base-uncased'),\n",
    "    \"t5\": TokenIndexer.by_name('pretrained_transformer_mismatched')('t5-base'),\n",
    "    \"char\": TokenIndexer.by_name('characters')(namespace=\"token_characters\", min_padding_length=5)\n",
    "}\n",
    "tgt_token_indexers = {'tgt_tokens': SingleIdTokenIndexer(namespace='tgt_tokens')}\n",
    "\n",
    "dataset_reader = SpiderASRRewriterReader_ILM_Combined_new(tables_json_fname=tables_json_fname,\n",
    "                                                    dataset_dir=dataset_dir,\n",
    "                                                    databases_dir=databases_dir,\n",
    "                                                    src_token_indexers=src_token_indexers,\n",
    "                                                    tgt_token_indexers=tgt_token_indexers,\n",
    "                                                    samples_limit=3)\n",
    "\n",
    "train_dataset = dataset_reader.read('train')\n",
    "\n",
    "dev_dataset = dataset_reader.read('dev')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': <allennlp.data.fields.text_field.TextField at 0x147ad8e10>,\n",
       " 'text_mask': <allennlp.data.fields.array_field.ArrayField at 0x14e814b40>,\n",
       " 'schema_mask': <allennlp.data.fields.array_field.ArrayField at 0x14e814c80>,\n",
       " 'schema_column_ids': <allennlp.data.fields.array_field.ArrayField at 0x14e814d20>,\n",
       " 'audio_feats': <allennlp.data.fields.list_field.ListField at 0x149578b90>,\n",
       " 'audio_mask': <allennlp.data.fields.array_field.ArrayField at 0x14e814cd0>,\n",
       " 'source_to_target': <allennlp.data.fields.namespace_swapping_field.NamespaceSwappingField at 0x14e810e60>,\n",
       " 'rewriter_tags': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x14a3523b0>,\n",
       " 'rewrite_seq': <allennlp.data.fields.text_field.TextField at 0x14e810eb0>,\n",
       " 'source_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x14e810c80>,\n",
       " 'target_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x14e810cd0>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x145ba8f50>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[@start@, are, [ANS], CCTV, [ANS], ?, [ANS], @end@]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10].fields['rewrite_seq'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[which,\n",
       " channels,\n",
       " air,\n",
       " not,\n",
       " owned,\n",
       " by,\n",
       " CC,\n",
       " TV,\n",
       " .,\n",
       " Give,\n",
       " me,\n",
       " the,\n",
       " channel,\n",
       " names,\n",
       " .,\n",
       " [SEP],\n",
       " broadcast,\n",
       " :,\n",
       " channel,\n",
       " id,\n",
       " ,,\n",
       " program,\n",
       " id,\n",
       " ,,\n",
       " time,\n",
       " of,\n",
       " day,\n",
       " .,\n",
       " broadcast,\n",
       " share,\n",
       " :,\n",
       " channel,\n",
       " id,\n",
       " ,,\n",
       " program,\n",
       " id,\n",
       " ,,\n",
       " date,\n",
       " ,,\n",
       " share,\n",
       " in,\n",
       " percent,\n",
       " .,\n",
       " channel,\n",
       " :,\n",
       " channel,\n",
       " id,\n",
       " ,,\n",
       " name,\n",
       " ,,\n",
       " owner,\n",
       " ,,\n",
       " share,\n",
       " in,\n",
       " percent,\n",
       " ,,\n",
       " rating,\n",
       " in,\n",
       " percent,\n",
       " .,\n",
       " program,\n",
       " :,\n",
       " program,\n",
       " id,\n",
       " ,,\n",
       " name,\n",
       " ,,\n",
       " origin,\n",
       " ,,\n",
       " launch,\n",
       " ,,\n",
       " owner,\n",
       " .]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10].fields['sentence'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcab6c2410104d6c9bf3451d50b27334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='building vocab', max=41, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  token_characters, Size: 54 || rewriter_tags, Size: 7 || tgt_tokens, Size: 28 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_index_to_token_vocabulary(\"token_characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp - Tokenization, Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': <allennlp.data.fields.text_field.TextField at 0x14ea0cbe0>,\n",
       " 'text_mask': <allennlp.data.fields.array_field.ArrayField at 0x14ea0cb90>,\n",
       " 'schema_mask': <allennlp.data.fields.array_field.ArrayField at 0x1a75d0f50>,\n",
       " 'schema_column_ids': <allennlp.data.fields.array_field.ArrayField at 0x1b0b5d050>,\n",
       " 'audio_feats': <allennlp.data.fields.list_field.ListField at 0x14b854190>,\n",
       " 'audio_mask': <allennlp.data.fields.array_field.ArrayField at 0x182bb6730>,\n",
       " 'source_to_target': <allennlp.data.fields.namespace_swapping_field.NamespaceSwappingField at 0x1a75defa0>,\n",
       " 'rewriter_tags': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x14a47a290>,\n",
       " 'rewrite_seq': <allennlp.data.fields.text_field.TextField at 0x1a75d9050>,\n",
       " 'source_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x1a75d90f0>,\n",
       " 'target_token_ids': <allennlp.data.fields.array_field.ArrayField at 0x1a75d9140>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x14b854e10>}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = copy(train_dataset[20])\n",
    "instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[What, are, the, chip, model]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence'].tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'mask', 'type_ids', 'offsets', 'wordpiece_mask'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['bert'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2054, 2024, 1996, 9090]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['bert']['token_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['bert']['type_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': [101, 2054, 2024, 1996, 9090, 2944, 102],\n",
       " 'mask': [True, True, True, True, True],\n",
       " 'type_ids': [0, 0, 0, 0, 0, 0, 0],\n",
       " 'offsets': [(1, 1), (2, 2), (3, 3), (4, 4), (5, 5)],\n",
       " 'wordpiece_mask': [True, True, True, True, True, True, True]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_token_indexers['bert'].tokens_to_indices(instance.fields['sentence'].tokens[:5], vocabulary=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'mask', 'type_ids', 'offsets', 'wordpiece_mask'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['t5'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([125, 130, 8, 336, 3056], [0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['t5']['token_ids'][:5], instance.fields['sentence']._indexed_tokens['t5']['type_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': [125, 130, 8, 336, 3056],\n",
       " 'mask': [True, True, True, True, True],\n",
       " 'type_ids': [0, 0, 0, 0, 0],\n",
       " 'offsets': [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
       " 'wordpiece_mask': [True, True, True, True, True]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_token_indexers['t5'].tokens_to_indices(instance.fields['sentence'].tokens[:5], vocabulary=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_characters'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['char'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 8, 25],\n",
       " [13, 4, 10, 17],\n",
       " [13, 2, 13, 24, 2, 7, 9],\n",
       " [6, 8, 2, 9],\n",
       " [3, 18, 2]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['char']['token_characters'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', '##de', 'f', '##g']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_token_indexers['bert']._tokenizer.tokenize('AbcdE FG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp - T5 indexer / embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_token_indexer = TokenIndexer.by_name('pretrained_transformer_mismatched')('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁ab', 'c', 'de', 'f']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_token_indexer._tokenizer.tokenize('abcdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = copy(train_dataset[0])\n",
    "instance.index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'mask', 'type_ids', 'offsets', 'wordpiece_mask'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fields['sentence']._indexed_tokens['t5'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': [125, 130, 8, 336, 3056],\n",
       " 'mask': [True, True, True, True, True],\n",
       " 'type_ids': [0, 0, 0, 0, 0],\n",
       " 'offsets': [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)],\n",
       " 'wordpiece_mask': [True, True, True, True, True]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_token_indexers['t5'].tokens_to_indices(instance.fields['sentence'].tokens[:5], vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger-ILM - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1423d3008a41f4a5beafab6d795664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='building vocab', max=111, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  token_characters, Size: 56 || rewriter_tags, Size: 8 || tgt_tokens, Size: 60 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '@@PADDING@@',\n",
       " 1: '@@UNKNOWN@@',\n",
       " 2: 'e',\n",
       " 3: 't',\n",
       " 4: 'a',\n",
       " 5: 'i',\n",
       " 6: 's',\n",
       " 7: 'n',\n",
       " 8: 'r',\n",
       " 9: 'd',\n",
       " 10: 'o',\n",
       " 11: ',',\n",
       " 12: 'c',\n",
       " 13: 'm',\n",
       " 14: 'l',\n",
       " 15: 'p',\n",
       " 16: 'h',\n",
       " 17: 'u',\n",
       " 18: 'g',\n",
       " 19: '.',\n",
       " 20: ':',\n",
       " 21: 'y',\n",
       " 22: 'f',\n",
       " 23: 'b',\n",
       " 24: 'w',\n",
       " 25: 'v',\n",
       " 26: 'k',\n",
       " 27: 'S',\n",
       " 28: 'z',\n",
       " 29: 'P',\n",
       " 30: 'E',\n",
       " 31: '[',\n",
       " 32: ']',\n",
       " 33: 'x',\n",
       " 34: '1',\n",
       " 35: '?',\n",
       " 36: 'j',\n",
       " 37: \"'\",\n",
       " 38: '2',\n",
       " 39: 'W',\n",
       " 40: 'A',\n",
       " 41: 'I',\n",
       " 42: 'U',\n",
       " 43: 'T',\n",
       " 44: '3',\n",
       " 45: 'B',\n",
       " 46: 'V',\n",
       " 47: '8',\n",
       " 48: '4',\n",
       " 49: '9',\n",
       " 50: 'C',\n",
       " 51: 'H',\n",
       " 52: 'N',\n",
       " 53: 'L',\n",
       " 54: 'F',\n",
       " 55: 'O'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_index_to_token_vocabulary(\"token_characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams \n",
    "TAG_EMB_DIM = 64\n",
    "SRC_EMB_DIM = 768 # BERT \n",
    "TGT_EMB_DIM = 300\n",
    "AUDIO_ENC_DIM = 128\n",
    "CHAR_EMB_DIM = 128\n",
    "\n",
    "ENCODER_DIM = 256\n",
    "TAGGING_FF_DIM = 64\n",
    "DECODER_DIM = ENCODER_DIM # It seems that otherwise it can't work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_embedder = BasicTextFieldEmbedder(\n",
    "        token_embedders={\n",
    "            \"bert\": TokenEmbedder.by_name(\"pretrained_transformer_mismatched\")(\"bert-base-uncased\"),\n",
    "            \"char\": TokenEmbedder.by_name(\"character_encoding\")(\n",
    "              # TokenCharactersEncoder(subclass of TokenEmbedder)\n",
    "              embedding=Embedding(embedding_dim=CHAR_EMB_DIM, vocab_namespace=\"token_characters\", num_embeddings=vocab.get_vocab_size('token_characters')),\n",
    "              encoder=Seq2VecEncoder.by_name(\"cnn\")(\n",
    "                embedding_dim=CHAR_EMB_DIM,\n",
    "                num_filters=4,\n",
    "                ngram_filter_sizes=[2, 3, 4, 5],\n",
    "                output_dim=CHAR_EMB_DIM,\n",
    "              ),\n",
    "              dropout=0.0,\n",
    "            ),\n",
    "        }\n",
    ")\n",
    "\n",
    "# tgt_text_embedder = BasicTextFieldEmbedder(\n",
    "#         token_embedders={\n",
    "#             \"tgt_tokens\": Embedding(\n",
    "#                 embedding_dim=TGT_EMB_DIM,\n",
    "#                 num_embeddings=vocab.get_vocab_size('tgt_tokens')\n",
    "#             )\n",
    "#         })\n",
    "tgt_text_embedder = Embedding(embedding_dim=TGT_EMB_DIM,\n",
    "                              num_embeddings=vocab.get_vocab_size('tgt_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embedder = Embedding(embedding_dim=TAG_EMB_DIM,\n",
    "                         vocab_namespace='rewriter_tags',\n",
    "                         vocab=vocab)\n",
    "# tag_embedder = Embedding(embedding_dim=TAG_EMB_DIM,\n",
    "#                          num_embeddings=vocab.get_vocab_size('rewriter_tags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_decoder = SeqDecoder.by_name('auto_regressive_seq_decoder')(\n",
    "    vocab=vocab,\n",
    "    decoder_net=DecoderNet.by_name('lstm_cell')(\n",
    "        decoding_dim=DECODER_DIM,\n",
    "        target_embedding_dim=TGT_EMB_DIM,\n",
    "        attention=Attention.by_name('bilinear')(DECODER_DIM, ENCODER_DIM)\n",
    "    ),\n",
    "    max_decoding_steps=100,\n",
    "    target_embedder=tgt_text_embedder,\n",
    "    target_namespace='tgt_tokens',\n",
    "    beam_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_s2v = CnnEncoder(embedding_dim = AUDIO_DIM_NO_POOLING,\n",
    "                       num_filters = 4,\n",
    "                       ngram_filter_sizes = (2, 3, 4, 5),\n",
    "                       output_dim = AUDIO_ENC_DIM)\n",
    "\n",
    "# lstm_s2s_no_tags = PytorchSeq2SeqWrapper(torch.nn.LSTM(SRC_EMB_DIM + AUDIO_ENC_DIM, ENCODER_DIM, batch_first=True))\n",
    "# lstm_s2s_with_tags = PytorchSeq2SeqWrapper(torch.nn.LSTM(SRC_EMB_DIM + AUDIO_ENC_DIM + TAG_EMB_DIM, ENCODER_DIM, batch_first=True))\n",
    "lstm_s2s = PytorchSeq2SeqWrapper(torch.nn.LSTM(SRC_EMB_DIM + AUDIO_ENC_DIM + TAG_EMB_DIM + CHAR_EMB_DIM, ENCODER_DIM, batch_first=True))\n",
    "\n",
    "# lstm_s2v_no_tags = PytorchSeq2VecWrapper(torch.nn.LSTM(ENCODER_DIM, ENCODER_DIM, batch_first=True))\n",
    "# lstm_s2v_with_tags = PytorchSeq2VecWrapper(torch.nn.LSTM(ENCODER_DIM, ENCODER_DIM, batch_first=True))\n",
    "\n",
    "# TODO: use s2s & s2v, instead of multilayer s2v, since we need sequence representations here \n",
    "\n",
    "# encoder_no_tags = SpeakQLEncoderV1(\n",
    "#     audio_attention_layer=CosineMatrixAttention(),\n",
    "#     audio_attention_residual='+',\n",
    "#     seq2seq_encoders=[lstm_s2s_no_tags],\n",
    "#     seq2vec_encoder=lstm_s2v_no_tags\n",
    "# )\n",
    "encoder_with_tags = SpeakQLEncoderV1(\n",
    "    audio_attention_layer=CosineMatrixAttention(),\n",
    "    audio_attention_residual='+',\n",
    "    seq2seq_encoders=[lstm_s2s],\n",
    "    seq2vec_encoder=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self._start_index: 3, @start@\n",
      "self._end_index: 4, @end@\n",
      "self._pad_index: 0, @@PADDING@@\n"
     ]
    }
   ],
   "source": [
    "# tagger_ILM_model = SpiderASRRewriter_Tagger_ILM(\n",
    "#     src_text_embedder=src_text_embedder,\n",
    "#     tag_embedder=tag_embedder,\n",
    "#     bert_pretrained_model='bert-base-uncased',\n",
    "#     audio_seq2vec_encoder=audio_s2v,\n",
    "#     encoder_no_tags=encoder_no_tags,\n",
    "#     encoder_with_tags=encoder_with_tags,\n",
    "#     rewrite_decoder=rewrite_decoder,\n",
    "#     ff_dimension=TAGGING_FF_DIM,\n",
    "#     concat_audio=True,\n",
    "#     vocab=vocab\n",
    "# )\n",
    "\n",
    "ILM_model = SpiderASRRewriter_ILM_Combined(\n",
    "    src_text_embedder=src_text_embedder,\n",
    "    rewriter_tag_embedder=tag_embedder,\n",
    "    use_tabert=False,\n",
    "    audio_seq2vec_encoder=audio_s2v,\n",
    "    encoder=encoder_with_tags,\n",
    "    rewrite_decoder=rewrite_decoder,\n",
    "    concat_audio=True,\n",
    "    vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b45b767b087426ba80e1b1d0e74237e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c80a7e39f4cd3bcec80e4a5ad8b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'peak_worker_0_memory_MB': 5002.682368,\n",
       " 'training_duration': '0:01:38.061549',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 0,\n",
       " 'epoch': 0,\n",
       " 'training_rewrite_seq_NLL': 4.076723575592041,\n",
       " 'training_loss': 4.076723575592041,\n",
       " 'training_worker_0_memory_MB': 5002.682368,\n",
       " 'validation_rewrite_seq_NLL': 4.064238548278809,\n",
       " 'validation_rewrite_seq_BLEU': 5.293757876399208e-14,\n",
       " 'validation_loss': 4.064238548278809,\n",
       " 'best_validation_rewrite_seq_NLL': 4.064238548278809,\n",
       " 'best_validation_rewrite_seq_BLEU': 5.293757876399208e-14,\n",
       " 'best_validation_loss': 4.064238548278809}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(ILM_model.parameters(), lr=0.01)\n",
    "\n",
    "train_dataset.index_with(vocab)\n",
    "dev_dataset.index_with(vocab)\n",
    "\n",
    "train_data_loader = PyTorchDataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "dev_data_loader = PyTorchDataLoader(dev_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "trainer = GradientDescentTrainer(model=ILM_model,\n",
    "                                 optimizer=optimizer,\n",
    "                                 data_loader=train_data_loader,\n",
    "                                 validation_data_loader=dev_data_loader,\n",
    "                                 patience=1,\n",
    "                                 num_epochs=1,\n",
    "                                 grad_norm=0.1,\n",
    "                                 cuda_device=-1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_characters': [[14, 5, 6, 3],\n",
       "  [3, 16, 2],\n",
       "  [12, 10, 6, 3],\n",
       "  [10, 22],\n",
       "  [2, 4, 12, 16],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [5, 7],\n",
       "  [3, 16, 2],\n",
       "  [12, 10, 8, 8, 2, 6, 15, 10, 7, 9, 5, 7, 18],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [3, 21, 15, 2],\n",
       "  [9, 2, 6, 12, 8, 5, 15, 3, 5, 10, 7],\n",
       "  [19],\n",
       "  [31, 27, 30, 29, 32],\n",
       "  [23, 8, 2, 2, 9, 6],\n",
       "  [20],\n",
       "  [23, 8, 2, 2, 9],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [23, 8, 2, 2, 9],\n",
       "  [7, 4, 13, 2],\n",
       "  [19],\n",
       "  [12, 16, 4, 8, 18, 2, 6],\n",
       "  [20],\n",
       "  [12, 16, 4, 8, 18, 2],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [12, 16, 4, 8, 18, 2],\n",
       "  [3, 21, 15, 2],\n",
       "  [11],\n",
       "  [12, 16, 4, 8, 18, 2],\n",
       "  [4, 13, 10, 17, 7, 3],\n",
       "  [19],\n",
       "  [9, 10, 18, 6],\n",
       "  [20],\n",
       "  [9, 10, 18],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [10, 24, 7, 2, 8],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [4, 23, 4, 7, 9, 10, 7, 2, 9],\n",
       "  [21, 2, 6],\n",
       "  [10, 8],\n",
       "  [7, 10],\n",
       "  [11],\n",
       "  [23, 8, 2, 2, 9],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [6, 5, 28, 2],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [7, 4, 13, 2],\n",
       "  [11],\n",
       "  [4, 18, 2],\n",
       "  [11],\n",
       "  [9, 4, 3, 2],\n",
       "  [10, 22],\n",
       "  [23, 5, 8, 3, 16],\n",
       "  [11],\n",
       "  [18, 2, 7, 9, 2, 8],\n",
       "  [11],\n",
       "  [24, 2, 5, 18, 16, 3],\n",
       "  [11],\n",
       "  [9, 4, 3, 2],\n",
       "  [4, 8, 8, 5, 25, 2, 9],\n",
       "  [11],\n",
       "  [9, 4, 3, 2],\n",
       "  [4, 9, 10, 15, 3, 2, 9],\n",
       "  [11],\n",
       "  [9, 4, 3, 2],\n",
       "  [9, 2, 15, 4, 8, 3, 2, 9],\n",
       "  [19],\n",
       "  [10, 24, 7, 2, 8, 6],\n",
       "  [20],\n",
       "  [10, 24, 7, 2, 8],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [22, 5, 8, 6, 3],\n",
       "  [7, 4, 13, 2],\n",
       "  [11],\n",
       "  [14, 4, 6, 3],\n",
       "  [7, 4, 13, 2],\n",
       "  [11],\n",
       "  [6, 3, 8, 2, 2, 3],\n",
       "  [11],\n",
       "  [12, 5, 3, 21],\n",
       "  [11],\n",
       "  [6, 3, 4, 3, 2],\n",
       "  [11],\n",
       "  [28, 5, 15],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [2, 13, 4, 5, 14],\n",
       "  [4, 9, 9, 8, 2, 6, 6],\n",
       "  [11],\n",
       "  [16, 10, 13, 2],\n",
       "  [15, 16, 10, 7, 2],\n",
       "  [11],\n",
       "  [12, 2, 14, 14],\n",
       "  [7, 17, 13, 23, 2, 8],\n",
       "  [19],\n",
       "  [15, 8, 10, 22, 2, 6, 6, 5, 10, 7, 4, 14, 6],\n",
       "  [20],\n",
       "  [15, 8, 10, 22, 2, 6, 6, 5, 10, 7, 4, 14],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [8, 10, 14, 2],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [22, 5, 8, 6, 3],\n",
       "  [7, 4, 13, 2],\n",
       "  [11],\n",
       "  [6, 3, 8, 2, 2, 3],\n",
       "  [11],\n",
       "  [12, 5, 3, 21],\n",
       "  [11],\n",
       "  [6, 3, 4, 3, 2],\n",
       "  [11],\n",
       "  [28, 5, 15],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [14, 4, 6, 3],\n",
       "  [7, 4, 13, 2],\n",
       "  [11],\n",
       "  [2, 13, 4, 5, 14],\n",
       "  [4, 9, 9, 8, 2, 6, 6],\n",
       "  [11],\n",
       "  [16, 10, 13, 2],\n",
       "  [15, 16, 10, 7, 2],\n",
       "  [11],\n",
       "  [12, 2, 14, 14],\n",
       "  [7, 17, 13, 23, 2, 8],\n",
       "  [19],\n",
       "  [6, 5, 28, 2, 6],\n",
       "  [20],\n",
       "  [6, 5, 28, 2],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [6, 5, 28, 2],\n",
       "  [9, 2, 6, 12, 8, 5, 15, 3, 5, 10, 7],\n",
       "  [19],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [3, 21, 15, 2, 6],\n",
       "  [20],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [3, 21, 15, 2],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [3, 21, 15, 2],\n",
       "  [9, 2, 6, 12, 8, 5, 15, 3, 5, 10, 7],\n",
       "  [19],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3, 6],\n",
       "  [20],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [9, 10, 18],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [15, 8, 10, 22, 2, 6, 6, 5, 10, 7, 4, 14],\n",
       "  [5, 9],\n",
       "  [11],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [3, 21, 15, 2],\n",
       "  [12, 10, 9, 2],\n",
       "  [11],\n",
       "  [9, 4, 3, 2],\n",
       "  [10, 22],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [11],\n",
       "  [12, 10, 6, 3],\n",
       "  [10, 22],\n",
       "  [3, 8, 2, 4, 3, 13, 2, 7, 3],\n",
       "  [19]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset[0].fields['sentence']._indexed_tokens['char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model \n",
    "\n",
    "tagger_ILM_model = Model.from_archive('runs/2.0.1/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger-ILM - Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another dataset_reader, excluding gold tags and rewriter_seq in the instances \n",
    "\n",
    "# No... should remove the fields in predictor, because we might want to evaluate \n",
    "# the rewrite seq predictions given oracle tagging, or even with teacher forcing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88215b59d254b218863a234d2805cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset_reader.read('test')\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': <allennlp.data.fields.text_field.TextField at 0x1770fa460>,\n",
       " 'text_mask': <allennlp.data.fields.array_field.ArrayField at 0x1770fadc0>,\n",
       " 'schema_mask': <allennlp.data.fields.array_field.ArrayField at 0x1770faaa0>,\n",
       " 'schema_column_ids': <allennlp.data.fields.array_field.ArrayField at 0x1770fa0a0>,\n",
       " 'audio_feats': <allennlp.data.fields.list_field.ListField at 0x189ba2090>,\n",
       " 'audio_mask': <allennlp.data.fields.array_field.ArrayField at 0x1770fa5a0>,\n",
       " 'rewriter_tags': <allennlp.data.fields.sequence_label_field.SequenceLabelField at 0x14205a830>,\n",
       " 'rewrite_seq': <allennlp.data.fields.text_field.TextField at 0x1770eb2d0>,\n",
       " 'metadata': <allennlp.data.fields.metadata_field.MetadataField at 0x189ba2f90>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_instance = Instance(test_dataset[0].fields.copy())\n",
    "_test_instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SpiderASRRewriterPredictor_Tagger_ILM(model=tagger_ILM_model,\n",
    "                                                  dataset_reader=dataset_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.predict_instance(_test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_output = predictor.predict_instance(_test_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 86)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Intermediate: make a dataset file with tagger predictor output as rewrite_tags, and feed to ILM predictor \n",
    "test_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_rewriter.json'\n",
    "tagger_output_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/local-test/output-2.2tL.json'\n",
    "\n",
    "tagger_output_jsons = []\n",
    "with open(test_path, 'r') as f:\n",
    "    test_dataset_json = json.load(f)\n",
    "with open(tagger_output_path, 'r') as f:\n",
    "    for l in f:\n",
    "        tagger_output_jsons.append(json.loads(l))\n",
    "\n",
    "len(test_dataset_json), len(tagger_output_jsons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 13)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_test_samples_by_oid = defaultdict(list)\n",
    "tagger_output_by_oid = defaultdict(list)\n",
    "\n",
    "for d in tagger_output_jsons:\n",
    "    o_id = d['original_id']\n",
    "    tagger_output_by_oid[o_id].append(d)\n",
    "\n",
    "for d in test_dataset_json:\n",
    "    if len(d) == 0:\n",
    "        continue\n",
    "        \n",
    "    o_id = d[0]['original_id']\n",
    "    \n",
    "    for c in d:\n",
    "        assert c['original_id'] == o_id \n",
    "        orig_test_samples_by_oid[o_id].append(c)\n",
    "\n",
    "len(orig_test_samples_by_oid), len(tagger_output_by_oid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_output_test_dataset = []\n",
    "\n",
    "for o_id, _outputs in tagger_output_by_oid.items():\n",
    "    _test_samples = orig_test_samples_by_oid[o_id]\n",
    "    assert len(_test_samples) == len(_outputs)\n",
    "    \n",
    "    d = []\n",
    "    for c, o in zip(_test_samples, _outputs):\n",
    "        assert ' '.join(c['question_toks']) == o['question']\n",
    "        _seq_len = len(c['question_toks'])\n",
    "        assert c['rewriter_tags'][:_seq_len] == o['gold_tags'][:_seq_len]\n",
    "        \n",
    "        c['tagger_predicted_rewriter_tags'] = o['tags_prediction']\n",
    "        d.append(c)\n",
    "    \n",
    "    tagger_output_test_dataset.append(d)\n",
    "\n",
    "len(tagger_output_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'span_ranges', 'original_id', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_score', 'question_toks_edit_distance', 'alignment_span_pairs', 'alignment_text_pairs', 'rewriter_tags', 'rewriter_edits', 'tagger_predicted_rewriter_tags'])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_output_test_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_test_path = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/local-test/test-rewriter-2.2tL.json'\n",
    "\n",
    "# with open(output_test_path, 'w') as f:\n",
    "#     json.dump(tagger_output_test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger-ILM - Eval (moved to ratsql-infer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align tags analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_json_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/train/train_rewriter.json'\n",
    "\n",
    "with open(train_json_path, 'r') as f:\n",
    "    train_json_list = json.load(f)\n",
    "\n",
    "len(train_json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36992a62df204e829dd51e4ff7579601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(train_json_list):\n",
    "    Get_align_tags(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT']\n",
      "['[SAME]', '[SAME]', '[SAME]', '[SAME]', '[SAME]', '[SAME]', '[DIFF+0]', '[SAME]', '[SAME]', '[SAME]', '[PUNCT]']\n"
     ]
    }
   ],
   "source": [
    "c = train_json_list[0][0]\n",
    "print(c['rewriter_tags'])\n",
    "print(c['align_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af04f84ec13548ab86cc89115c5380f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['B-DEL', 'B-EDIT', 'I-DEL', 'I-EDIT', 'L-DEL', 'L-EDIT', 'O-KEEP', 'U-DEL', 'U-EDIT']\n",
      "['[DIFF+0]', '[DIFF+1]', '[DIFF+2]', '[DIFF+3]', '[DIFF+4]', '[DIFF-1]', '[DIFF-2]', '[DIFF-3]', '[DIFF-4]', '[PUNCT]', '[SAME]']\n"
     ]
    }
   ],
   "source": [
    "all_rewriter_tags = set()\n",
    "all_align_tags = set()\n",
    "\n",
    "for d in tqdm(train_json_list):\n",
    "    for c in d:\n",
    "        all_rewriter_tags.update(c['rewriter_tags'])\n",
    "        all_align_tags.update(c['align_tags'])\n",
    "\n",
    "all_rewriter_tags = sorted(list(all_rewriter_tags))\n",
    "all_align_tags = sorted(list(all_align_tags))\n",
    "print(all_rewriter_tags)\n",
    "print(all_align_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-DEL': 0, 'B-EDIT': 1, 'I-DEL': 2, 'I-EDIT': 3, 'L-DEL': 4, 'L-EDIT': 5, 'O-KEEP': 6, 'U-DEL': 7, 'U-EDIT': 8}\n",
      "{'[DIFF+0]': 0, '[DIFF+1]': 1, '[DIFF+2]': 2, '[DIFF+3]': 3, '[DIFF+4]': 4, '[DIFF-1]': 5, '[DIFF-2]': 6, '[DIFF-3]': 7, '[DIFF-4]': 8, '[PUNCT]': 9, '[SAME]': 10}\n"
     ]
    }
   ],
   "source": [
    "rewriter_tag2idx = {t : i for i, t in enumerate(all_rewriter_tags)}\n",
    "align_tag2idx = {t : i for i, t in enumerate(all_align_tags)}\n",
    "print(rewriter_tag2idx)\n",
    "print(align_tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0363ad74f56245e1a82b72c78b84820c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    49,     50,     29,     36,     82,     74,     59,     74,\n",
       "           259,    610,    331],\n",
       "       [   395,    620,    341,    569,   1048,    748,    643,    904,\n",
       "          2331,   1365,   5708],\n",
       "       [    10,     27,     12,     21,     41,     13,     16,     25,\n",
       "           134,    120,    102],\n",
       "       [    59,     92,     37,     83,    179,     90,     42,     98,\n",
       "           225,    366,    781],\n",
       "       [    19,     41,      7,     36,     54,     58,     38,     86,\n",
       "           270,    732,    312],\n",
       "       [   433,    668,    417,    477,    837,    704,    598,    915,\n",
       "          2925,   1360,   5338],\n",
       "       [  5834,   9289,   9030,   9418,  17670,   5182,   2168,   2206,\n",
       "          3017,  30047, 394062],\n",
       "       [   118,    170,     62,    127,    289,    258,    278,    421,\n",
       "          1902,   7152,    910],\n",
       "       [  1600,   2426,   1368,   1810,   2849,   3905,   3578,   5565,\n",
       "         17624,  12040,  14469]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurrence = np.zeros((len(all_rewriter_tags), len(all_align_tags)), dtype=int)\n",
    "\n",
    "for d in tqdm(train_json_list):\n",
    "    for c in d:\n",
    "        for _rt, _at in zip(c['rewriter_tags'], c['align_tags']):\n",
    "            _ri = rewriter_tag2idx[_rt]\n",
    "            _ai = align_tag2idx[_at]\n",
    "            cooccurrence[_ri, _ai] += 1\n",
    "\n",
    "cooccurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[DIFF+0]  [DIFF+1]  [DIFF+2]  [DIFF+3]  [DIFF+4]  [DIFF-1]  [DIFF-2]  [DIFF-3]  [DIFF-4]  [PUNCT]   [SAME]    \n",
      "B-DEL\t        49        50        29        36        82        74        59        74       259       610       331\n",
      "B-EDIT\t       395       620       341       569      1048       748       643       904      2331      1365      5708\n",
      "I-DEL\t        10        27        12        21        41        13        16        25       134       120       102\n",
      "I-EDIT\t        59        92        37        83       179        90        42        98       225       366       781\n",
      "L-DEL\t        19        41         7        36        54        58        38        86       270       732       312\n",
      "L-EDIT\t       433       668       417       477       837       704       598       915      2925      1360      5338\n",
      "O-KEEP\t      5834      9289      9030      9418     17670      5182      2168      2206      3017     30047    394062\n",
      "U-DEL\t       118       170        62       127       289       258       278       421      1902      7152       910\n",
      "U-EDIT\t      1600      2426      1368      1810      2849      3905      3578      5565     17624     12040     14469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t' + ''.join([f'{_at:<10s}' for _at in all_align_tags]))\n",
    "for i in range(len(all_rewriter_tags)):\n",
    "    print(all_rewriter_tags[i] + '\\t' + \"\".join([f'{cnt:10d}' for cnt in cooccurrence[i]]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  [DIFF+0]  [DIFF+1]  [DIFF+2]  [DIFF+3]  [DIFF+4]  [DIFF-1]  [DIFF-2]  [DIFF-3]  [DIFF-4]   [PUNCT]    [SAME]\n",
      "KEEP\t      5834      9289      9030      9418     17670      5182      2168      2206      3017     30047    394062\n",
      "DEL\t       196       288       110       220       466       403       391       606      2565      8614      1655\n",
      "EDIT\t      2487      3806      2163      2939      4913      5447      4861      7482     23105     15131     26296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rew_keep_ids = [i for i, t in enumerate(all_rewriter_tags) if t.endswith('KEEP')]\n",
    "rew_del_ids = [i for i, t in enumerate(all_rewriter_tags) if t.endswith('DEL')]\n",
    "rew_edit_ids = [i for i, t in enumerate(all_rewriter_tags) if t.endswith('EDIT')]\n",
    "\n",
    "print('\\t' + ''.join([f'{_at:>10s}' for _at in all_align_tags]))\n",
    "print('KEEP\\t' + \"\".join([f'{cnt:10d}' for cnt in cooccurrence[rew_keep_ids].sum(0)]))\n",
    "print('DEL\\t' + \"\".join([f'{cnt:10d}' for cnt in cooccurrence[rew_del_ids].sum(0)]))\n",
    "print('EDIT\\t' + \"\".join([f'{cnt:10d}' for cnt in cooccurrence[rew_edit_ids].sum(0)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq rewriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq - Dataset Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset_readers.rewriter_s2s_tabert_reader)\n",
    "importlib.reload(models.rewriter_s2s_tabert)\n",
    "\n",
    "from dataset_readers.rewriter_s2s_tabert_reader import SpiderASRRewriterReader_Seq2seq_TaBERT\n",
    "from models.rewriter_s2s_tabert import SpiderASRRewriter_Seq2seq_TaBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7542d8d0de054ec6a460def062ddd626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7db48252804c058b0b089e4941c094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='reading instances', max=1, style=ProgressSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tables_json_fname = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json'\n",
    "dataset_dir = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my'\n",
    "databases_dir = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database'\n",
    "tabert_model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/TaBERT/pretrained-models/tabert_base_k1/model.bin'\n",
    "\n",
    "\n",
    "src_token_indexers = {'bert': TokenIndexer.by_name('pretrained_transformer_mismatched')('bert-base-uncased')}\n",
    "tgt_token_indexers = {'tgt_tokens': SingleIdTokenIndexer(namespace='tgt_tokens')}\n",
    "\n",
    "dataset_reader_s2s = SpiderASRRewriterReader_Seq2seq_TaBERT(tables_json_fname=tables_json_fname,\n",
    "                                                            dataset_dir=dataset_dir,\n",
    "                                                            databases_dir=databases_dir,\n",
    "                                                            tabert_model_path=tabert_model_path,\n",
    "                                                            src_token_indexers=src_token_indexers,\n",
    "                                                            tgt_token_indexers=tgt_token_indexers,\n",
    "                                                            debug=True)\n",
    "\n",
    "train_dataset_s2s = dataset_reader_s2s.read('train')\n",
    "\n",
    "dev_dataset_s2s = dataset_reader_s2s.read('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset_s2s), len(dev_dataset_s2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dataset_s2s[0].fields['rewrite_seq_s2s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57597ded06cb4aad93c8c74ed71392f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='building vocab', max=426, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tgt_tokens, Size: 328 || Non Padded Namespaces: {'*tags', '*labels'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset_s2s + dev_dataset_s2s)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams \n",
    "TAG_EMB_DIM = 64\n",
    "SRC_EMB_DIM = 768 # BERT \n",
    "TGT_EMB_DIM = 300\n",
    "AUDIO_ENC_DIM = 128\n",
    "\n",
    "ENCODER_DIM = 256\n",
    "TAGGING_FF_DIM = 64\n",
    "DECODER_DIM = ENCODER_DIM # It seems that otherwise it can't work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_text_embedder = BasicTextFieldEmbedder(\n",
    "#         token_embedders={\n",
    "#             \"bert\": TokenEmbedder.by_name(\"pretrained_transformer_mismatched\")(\"bert-base-uncased\")\n",
    "#         })\n",
    "src_text_embedder = None\n",
    "\n",
    "tgt_text_embedder = Embedding(embedding_dim=TGT_EMB_DIM,\n",
    "                              num_embeddings=vocab.get_vocab_size('tgt_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_decoder = SeqDecoder.by_name('speakql_copynet_seq_decoder')(\n",
    "    vocab=vocab,\n",
    "    encoder_output_dim=ENCODER_DIM,\n",
    "    is_bidirectional_input=True,\n",
    "    attention=Attention.by_name('cosine')(),\n",
    "    beam_size=4,\n",
    "    max_decoding_steps=100,\n",
    "    target_embedder=tgt_text_embedder,\n",
    "    target_namespace='tgt_tokens'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_s2v = CnnEncoder(embedding_dim = AUDIO_DIM_NO_POOLING,\n",
    "                       num_filters = 4,\n",
    "                       ngram_filter_sizes = (2, 3, 4, 5),\n",
    "                       output_dim = AUDIO_ENC_DIM)\n",
    "\n",
    "lstm_s2s = PytorchSeq2SeqWrapper(torch.nn.LSTM(SRC_EMB_DIM + AUDIO_ENC_DIM, ENCODER_DIM, batch_first=True))\n",
    "lstm_s2v = PytorchSeq2VecWrapper(torch.nn.LSTM(ENCODER_DIM, ENCODER_DIM, batch_first=True))\n",
    "\n",
    "# TODO: use s2s & s2v, instead of multilayer s2v, since we need sequence representations here \n",
    "\n",
    "encoder_for_s2s = SpeakQLEncoderV1(\n",
    "    audio_attention_layer=CosineMatrixAttention(),\n",
    "    audio_attention_residual='+',\n",
    "    seq2seq_encoders=[lstm_s2s],\n",
    "    seq2vec_encoder=lstm_s2v\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dataset_readers.rewriter_s2s_tabert_reader)\n",
    "importlib.reload(models.rewriter_s2s_tabert)\n",
    "\n",
    "from dataset_readers.rewriter_s2s_tabert_reader import SpiderASRRewriterReader_Seq2seq_TaBERT\n",
    "from models.rewriter_s2s_tabert import SpiderASRRewriter_Seq2seq_TaBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2s_model = SpiderASRRewriter_Seq2seq_TaBERT(\n",
    "    src_text_embedder=src_text_embedder,\n",
    "    tabert_model_path=tabert_model_path,\n",
    "    finetune_tabert=False,\n",
    "    audio_seq2vec_encoder=audio_s2v,\n",
    "    encoder=encoder_for_s2s,\n",
    "    rewrite_decoder=rewrite_decoder,\n",
    "    concat_audio=True,\n",
    "    vocab=vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_s2s.index_with(vocab)\n",
    "dev_dataset_s2s.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = PyTorchDataLoader(train_dataset_s2s, batch_size=8, shuffle=True)\n",
    "dev_data_loader = PyTorchDataLoader(dev_dataset_s2s, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _b in tqdm(train_data_loader.__iter__()):\n",
    "    s2s_model(**_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(s2s_model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = GradientDescentTrainer(model=s2s_model,\n",
    "                                 optimizer=optimizer,\n",
    "                                 data_loader=train_data_loader,\n",
    "                                 validation_data_loader=dev_data_loader,\n",
    "                                 patience=1,\n",
    "                                 num_epochs=1,\n",
    "                                 grad_norm=0.1,\n",
    "                                 cuda_device=-1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq - Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_s2s = dataset_reader_s2s.read('test')\n",
    "len(test_dataset_s2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_instance = Instance(test_dataset_s2s[0].fields.copy())\n",
    "_test_instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SpiderASRRewriterPredictor_Seq2seq(model=s2s_model,\n",
    "                                               dataset_reader=dataset_reader_s2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict_instance(_test_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq - Analysis (moved to ratsql-infer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriter preds -> Reranker cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_rewrt_cand_span_ranges(\n",
    "    orig_tokens,\n",
    "    orig_span_ranges,\n",
    "    tags,\n",
    "    ILM_tokens):\n",
    "    \n",
    "    _edits = []\n",
    "    _curr_edit = []\n",
    "    for tok in ILM_tokens:\n",
    "        if tok == '[ANS]':\n",
    "            _edits.append(_curr_edit)\n",
    "            _curr_edit = []\n",
    "        elif tok == END_SYMBOL:  # Allennlp END_SYMBOL \n",
    "            break\n",
    "        else:\n",
    "            _curr_edit.append(tok)\n",
    "    \n",
    "    # Get the span ranges, combining each edit span together \n",
    "    _coarse_span_ranges = [] # (st, ed) for each edit span \n",
    "    _is_edit_spans = []      # True for edit spans, False for other tokens \n",
    "\n",
    "    _span_st = 0\n",
    "    _span_ed = 0\n",
    "    \n",
    "    for i, (st, ed) in enumerate(orig_span_ranges):\n",
    "        if tags[i].endswith('KEEP'):\n",
    "            # Not an edit span \n",
    "            _coarse_span_ranges.append((st, ed))\n",
    "            _is_edit_spans.append(False)\n",
    "            \n",
    "        elif sum(_is_edit_spans) >= len(_edits):\n",
    "            # Is an edit span, but no more edits available, treat as DEL\n",
    "            # (TODO: in original processing, should treat as KEEP!) \n",
    "            pass\n",
    "        \n",
    "        elif (tags[i] == 'U-EDIT'):\n",
    "            _coarse_span_ranges.append((st, ed))\n",
    "            _is_edit_spans.append(True)\n",
    "            \n",
    "        elif (tags[i] == 'B-EDIT'):\n",
    "            if type(st) == type(ed) == str:\n",
    "                # actual token \n",
    "                _span_st = st\n",
    "                _span_ed = ed\n",
    "        \n",
    "        elif (tags[i] == 'I-EDIT'):\n",
    "            assert type(_span_st) == type(_span_ed)\n",
    "            \n",
    "            if type(st) == type(ed) == str:\n",
    "                # actual token \n",
    "                if _span_st == _span_ed == 0:\n",
    "                    # prev tokens are puncts \n",
    "                    _span_st = st\n",
    "                    _span_ed = ed\n",
    "                else:\n",
    "                    # prev have actual tokens, update ed \n",
    "                    _span_ed = ed\n",
    "            \n",
    "        elif (tags[i] == 'L-EDIT'):\n",
    "            assert type(_span_st) == type(_span_ed)\n",
    "            \n",
    "            if type(st) == type(ed) == str:\n",
    "                # actual token \n",
    "                if _span_st == _span_ed == 0:\n",
    "                    # prev tokens are puncts \n",
    "                    _span_st = st\n",
    "                    _span_ed = ed\n",
    "                else:\n",
    "                    # prev have actual tokens, update ed \n",
    "                    _span_ed = ed\n",
    "            \n",
    "            _coarse_span_ranges.append((_span_st, _span_ed))\n",
    "            _is_edit_spans.append(True)\n",
    "            _span_st = 0\n",
    "            _span_ed = 0\n",
    "        \n",
    "        elif tags[i].endswith('DEL'):\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            print('Unknown tag: {}'.format(tags[i]))\n",
    "\n",
    "    assert sum(_is_edit_spans) <= len(_edits)\n",
    "    \n",
    "    # Split each edit span range by number of chars (excluding puncts) \n",
    "    out_span_ranges = []\n",
    "    \n",
    "    _eid = 0\n",
    "    for i, (st, ed) in enumerate(_coarse_span_ranges):\n",
    "        # st, ed are 0 or str!\n",
    "        if not _is_edit_spans[i]:\n",
    "            out_span_ranges.append((st, ed))\n",
    "            continue\n",
    "        \n",
    "        # is an edit span; split \n",
    "        _edit = _edits[_eid]\n",
    "        _eid += 1\n",
    "        \n",
    "        if st == ed == 0:\n",
    "            # original token is punct, no time span \n",
    "            out_span_ranges.extend([(0, 0)] * len(_edit))\n",
    "            continue\n",
    "        \n",
    "        _token_lens = []\n",
    "        for tok in _edit:\n",
    "            _nchars = len([_c for _c in tok if _c not in string.punctuation])\n",
    "            _token_lens.append(_nchars)\n",
    "        \n",
    "        if sum(_token_lens) == 0:\n",
    "            # all edited tokens are puncts \n",
    "            out_span_ranges.extend([(0, 0)] * len(_edit))\n",
    "            continue\n",
    "        \n",
    "        _unit_time = (float(ed) - float(st)) / sum(_token_lens)\n",
    "        assert _unit_time > 0, \\\n",
    "            f'{orig_tokens}\\n{orig_span_ranges}\\n{tags}\\n{ILM_tokens}\\n\\n{_coarse_span_ranges}\\n{_is_edit_spans}'\n",
    "        \n",
    "        _st = _ed = float(st)\n",
    "        for _l in _token_lens:\n",
    "            if _l == 0:\n",
    "                # this is punct\n",
    "                out_span_ranges.append((0, 0))\n",
    "                continue\n",
    "            # this is not punct \n",
    "            _st = _ed\n",
    "            _ed = _st + _l * _unit_time\n",
    "            out_span_ranges.append((f'{_st:.4f}', f'{_ed:.4f}'))\n",
    "        assert np.isclose(_ed, float(ed)), f'{_ed} != {ed}'\n",
    "        \n",
    "    return out_span_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'U-EDIT', 'B-EDIT', 'I-EDIT', 'L-EDIT', 'U-EDIT', 'U-EDIT']\n",
      "Edits: [['b', ','], ['cde', '&', 'yu-iop'], ['f', 'g', 'h']]\n",
      "['a', 'b', ',', 'cde', '&', 'yu-iop', 'f', 'g', 'h']\n",
      "\n",
      "[('0.1', '0.2'), (0, 0), (0, 0), ('0.2000', '0.3500'), (0, 0), ('0.3500', '0.6000'), ('0.7000', '0.8000'), ('0.8000', '0.9000'), ('0.9000', '1.0000')]\n"
     ]
    }
   ],
   "source": [
    "orig_tokens = 'a , c d e fgh i'.split(' ')\n",
    "orig_span_ranges = [\n",
    "    (\"0.1\", \"0.2\"),\n",
    "    (0, 0),\n",
    "    (\"0.2\", \"0.3\"),\n",
    "    (\"0.3\", \"0.5\"),\n",
    "    (\"0.5\", \"0.6\"),\n",
    "    (\"0.7\", \"1.0\"),\n",
    "    (\"1.0\", \"1.2\"),\n",
    "]\n",
    "tags = ['O-KEEP', 'U-EDIT', 'B-EDIT', 'I-EDIT', 'L-EDIT', 'U-EDIT', 'U-EDIT']\n",
    "ILM_tokens = 'b , [ANS] cde & yu-iop [ANS] f g h [ANS] @end@'.split(' ')\n",
    "\n",
    "print(Postprocess_rewrite_seq(tags=tags,\n",
    "                              rewrite_seq=ILM_tokens,\n",
    "                              question_toks=orig_tokens))\n",
    "print()\n",
    "print(Generate_rewrt_cand_span_ranges(\n",
    "        orig_tokens,\n",
    "        orig_span_ranges,\n",
    "        tags,\n",
    "        ILM_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITER_VERSION = '2.5.0.0t-2.4.0.0i' # Has to be taggerILM \n",
    "HUMAN_TEST = True\n",
    "\n",
    "if not HUMAN_TEST:\n",
    "    rewriter_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/{REWRITER_VERSION}.json'\n",
    "    reranker_test_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/test_reranker.json'\n",
    "    reranker_extra_cands_test_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/test_reranker_with_{REWRITER_VERSION}.json'\n",
    "else:\n",
    "    rewriter_output_path = f'/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/outputs/ratsql-test-save/humantest-yshao-{REWRITER_VERSION}.json'\n",
    "    reranker_test_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/human_test/human_test_yshao_reranker.json'\n",
    "    reranker_extra_cands_test_path = f'/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/my/dev/aggreg_extra_cands/human_test_yshao_reranker_with_{REWRITER_VERSION}.json'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(rewriter_output_path, 'r') as f:\n",
    "    rewriter_outputs = json.load(f)\n",
    "with open(reranker_test_path, 'r') as f:\n",
    "    reranker_test_samples = json.load(f)\n",
    "    \n",
    "len(rewriter_outputs), len(reranker_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'original_id', 'span_ranges', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance', 'alignment_span_pairs', 'alignment_text_pairs', 'rewriter_tags', 'rewriter_edits', 'pred_tags', 'pred_ILM', 'pred_ILM_cands', 'rewritten_question', 'pred_sql', 'score', 'exact']),\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'original_id', 'span_ranges', 'ratsql_pred_sql', 'gold_question', 'gold_question_toks', 'ratsql_pred_exact', 'ratsql_pred_score', 'question_toks_edit_distance']))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewriter_outputs[0][0].keys(), reranker_test_samples[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'B-EDIT', 'I-EDIT', 'I-EDIT', 'I-EDIT', 'L-EDIT', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: [['US', 'museum', '?']]\n",
      "--- Not enough edits ---\n",
      "Tags: ['O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'O-KEEP', 'U-EDIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Edits: []\n"
     ]
    }
   ],
   "source": [
    "for d_out, d_orig in zip(rewriter_outputs, reranker_test_samples):\n",
    "    _question_set = set([' '.join(c['question_toks']) for c in d_orig])\n",
    "    \n",
    "    for c in d_orig:\n",
    "        c['from_rewriter'] = False\n",
    "    \n",
    "    _add_cands = []\n",
    "    for i in range(1):\n",
    "        # Add rewrites from c_out \n",
    "        # (Currently only 1st cands are predicted)\n",
    "        \n",
    "        c_out = d_out[i]\n",
    "        c_orig = d_orig[i]\n",
    "        \n",
    "        # for k in c_orig:\n",
    "        #     assert c_out[k] == c_orig[k], f'{k}, {c_out[k]}, {c_orig[k]}'\n",
    "        \n",
    "        for _pred_ILM_cand in c_out['pred_ILM_cands']:\n",
    "            _rewritten_question_toks = Postprocess_rewrite_seq(\n",
    "                tags=c_out['pred_tags'],\n",
    "                rewrite_seq=_pred_ILM_cand,\n",
    "                question_toks=c_out['question_toks'],\n",
    "            )\n",
    "\n",
    "            _rewritten_question = ' '.join(_rewritten_question_toks)\n",
    "            if _rewritten_question in _question_set:\n",
    "                # This cand already exists \n",
    "                continue\n",
    "\n",
    "            _rewrt_span_ranges = Generate_rewrt_cand_span_ranges(\n",
    "                orig_tokens=c_out['question_toks'],\n",
    "                orig_span_ranges=c_out['span_ranges'],\n",
    "                tags=c_out['pred_tags'],\n",
    "                ILM_tokens=_pred_ILM_cand,\n",
    "            )\n",
    "\n",
    "            assert len(_rewrt_span_ranges) == len(_rewritten_question_toks), \\\n",
    "                f\"{_rewrt_span_ranges}\\n{_rewritten_question_toks}\"\n",
    "\n",
    "            c_add = deepcopy(c_orig)\n",
    "            c_add['question'] = _rewritten_question\n",
    "            c_add['question_toks'] = _rewritten_question_toks\n",
    "            c_add['span_ranges'] = _rewrt_span_ranges\n",
    "            c_add['ratsql_pred_sql'] = None\n",
    "            c_add['ratsql_pred_exact'] = None\n",
    "            c_add['ratsql_pred_score'] = None\n",
    "            c_add['question_toks_edit_distance'] = None\n",
    "            c_add['from_rewriter'] = True\n",
    "\n",
    "            _add_cands.append(c_add)\n",
    "            _question_set.add(c_add['question'])\n",
    "    \n",
    "    d_orig.extend(_add_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewriter_outputs[3]), len(reranker_test_samples[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the idea off the path owned by the student whose last name is Smith ?',\n",
       " \"What is the idea off the path owned by the student who's last name is Smith .\",\n",
       " 'What is the idea off the path owned by the students whose last name is Smith ?',\n",
       " 'What is the idea off the pad owned by the student whose last name is Smith .',\n",
       " \"What is the idea off the pad owned by the student who's last name is Smith .\",\n",
       " 'What is the idea off the pad owned by the students whose last name is Smith ?',\n",
       " \"What is the idea off the path owned by the students who's last name is Smith .\"]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(c['question_toks']) for c in rewriter_outputs[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the idea off the path owned by the student whose last name is Smith ?',\n",
       " \"What is the idea off the path owned by the student who's last name is Smith .\",\n",
       " 'What is the idea off the path owned by the students whose last name is Smith ?',\n",
       " 'What is the idea off the pad owned by the student whose last name is Smith .',\n",
       " \"What is the idea off the pad owned by the student who's last name is Smith .\",\n",
       " 'What is the idea off the pad owned by the students whose last name is Smith ?',\n",
       " \"What is the idea off the path owned by the students who's last name is Smith .\",\n",
       " 'What is the id off the path owned by the student whose last name is Smith ?',\n",
       " 'What is the the off the path owned by the student whose last name is Smith ?',\n",
       " 'What is the and off the path owned by the student whose last name is Smith ?']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(c['question_toks']) for c in reranker_test_samples[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(reranker_extra_cands_test_path, 'w') as f:\n",
    "    json.dump(reranker_test_samples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty span?\n",
    "for d in reranker_test_samples:\n",
    "    for c in d:\n",
    "        for st, ed in c['span_ranges']:\n",
    "            assert (st == ed == 0) or (float(ed) - float(st) >= 1e-4), c['span_ranges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = copy(train_dataset[0])\n",
    "instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(instance.fields['tags'].labels, instance.fields['tags']._indexed_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.fields['sentence']._indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(instance.fields['rewrite_seq'].tokens, instance.fields['rewrite_seq']._indexed_tokens['tgt_tokens']['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.fields['rewrite_seq']._indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Instance.add_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = Instance(test_dataset[0].fields.copy())\n",
    "instance.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instance.fields['rewrite_seq'].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = instance.fields['rewrite_seq'].tokens\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([1])[0]\n",
    "b = torch.LongTensor([2])[0]\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.LongTensor([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = read_dataset_schema('/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/tables.json')\n",
    "len(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas['perpetrator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(c.name, c.text) for c in schemas['perpetrator']['people'].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_gnn.spider.utils.read_dataset_values\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "db_id = 'perpetrator'\n",
    "dataset_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database'\n",
    "tables = [\"perpetrator\", \"people\"]\n",
    "\n",
    "db = os.path.join(dataset_path, db_id, db_id + \".sqlite\")\n",
    "try:\n",
    "    conn = sqlite3.connect(db)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Can't connect to SQL: {e} in path {db}\")\n",
    "conn.text_factory = str\n",
    "cursor = conn.cursor()\n",
    "\n",
    "values = {}\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT * FROM {table} LIMIT 5000\")\n",
    "        values[table] = cursor.fetchall()\n",
    "    except:\n",
    "        conn.text_factory = lambda x: str(x, 'latin1')\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM {table} LIMIT 5000\")\n",
    "        values[table] = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM sqlite_master where type='table'\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f\"SELECT * FROM people LIMIT 5000\")\n",
    "[d[0] for d in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('PRAGMA TABLE_INFO(people)')\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[0] for d in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = OrderedDict({'4': 4, '2': 2, '0': 0})\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = OrderedDict(sorted(d.items(), key=lambda x : x[1] % 3))\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2787, 1.1783, 2.0894, 0.1343],\n",
       "         [2.4704, 1.2044, 1.3239, 1.3491]]),\n",
       " tensor([[-0.0047,  0.9896,  0.2813,  2.1478],\n",
       "         [ 0.7637,  2.0747,  0.1790, -1.4233]]),\n",
       " tensor([[ 1., -1.],\n",
       "         [-1.,  1.]]))"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CosineEmbeddingLoss \n",
    "arr1 = np.random.randn(2, 4) + np.ones((2, 4))\n",
    "arr2 = np.random.randn(2, 4) + np.ones((2, 4))\n",
    "tensor1 = torch.tensor(arr1, dtype=torch.float32)\n",
    "tensor2 = torch.tensor(arr2, dtype=torch.float32)\n",
    "y = torch.tensor(np.eye(2) * 2 - np.ones((2, 2)), dtype=tensor1.dtype, device=tensor1.device)\n",
    "tensor1, tensor2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([2, 4]))"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.size(), tensor2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_loss = CosineEmbeddingLoss(margin=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.4972, 2.0322, 1.4485, 1.8441],\n",
       "         [0.4712, 1.2000, 1.9665, 1.2432],\n",
       "         [2.4972, 2.0322, 1.4485, 1.8441],\n",
       "         [0.4712, 1.2000, 1.9665, 1.2432]]),\n",
       " tensor([[1.8994, 1.5220, 1.3646, 0.9790],\n",
       "         [1.8994, 1.5220, 1.3646, 0.9790],\n",
       "         [1.2207, 0.7970, 1.5540, 0.8197],\n",
       "         [1.2207, 0.7970, 1.5540, 0.8197]]),\n",
       " tensor([[ 1., -1.],\n",
       "         [-1.,  1.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tensor1.unsqueeze(0).expand(2, 2, 4).reshape(4, 4)\n",
    "t2 = tensor2.unsqueeze(1).expand(2, 2, 4).reshape(4, 4)\n",
    "y_ = y.view(4)\n",
    "t1, t2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4648)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_loss(t1, t2, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46483609747567395"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cos_sim\n",
    "l = cos_sim(arr1[0], arr2[0]) + cos_sim(arr1[1], arr2[1])\n",
    "l += (1 - cos_sim(arr1[0], arr2[1])) + (1 - cos_sim(arr1[1], arr2[0]))\n",
    "l /= 4.0\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "model_ckpt_bin = '/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SpeakQL/SpeakQL/Allennlp_models/runs/3.1.0/model.tar.gz'\n",
    "\n",
    "with tarfile.open(model_ckpt_bin, 'r:gz') as tar:\n",
    "    f = tar.extractfile('weights.th')\n",
    "    ckpt = torch.load(f, map_location=torch.device('cpu'))\n",
    "\n",
    "type(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tabert_model._bert_model.bert.embeddings.word_embeddings.weight',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.2.output.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.7.output.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'tabert_model._bert_model.cls.predictions.transform.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.3.output.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.8.output.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'tabert_embedder.tabert_model._bert_model.bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'audio_seq2vec_encoder.conv_layer_0.weight',\n",
       " 'encoder.seq2seq_encoders.0._module.weight_ih_l0',\n",
       " 'encoder.seq2vec_encoder._module.bias_ih_l0',\n",
       " 'rewrite_decoder._decoder_cell.weight_hh']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ckpt.keys())[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1329.27 MiB, increment: 0.56 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7312,  1.1718, -0.9274,  0.5451,  0.0663],\n",
       "          [-0.4370,  0.7626,  0.4415, -0.0091, -0.8425],\n",
       "          [ 0.1374,  0.9386, -0.1860, -0.6446,  0.4100]],\n",
       " \n",
       "         [[ 0.4085,  0.2579,  1.0950, -0.5065,  0.0998],\n",
       "          [-0.6540,  0.7317, -1.4567,  1.6089,  0.0938],\n",
       "          [-1.2597,  0.2546, -0.5020, -1.0412,  0.7323]]]),\n",
       " tensor([[[ 1.0000,  0.0458,  0.4858],\n",
       "          [ 0.0458,  1.0000,  0.1467],\n",
       "          [ 0.4858,  0.1467,  1.0000]],\n",
       " \n",
       "         [[ 1.0000, -0.7979, -0.1627],\n",
       "          [-0.7979,  1.0000,  0.0301],\n",
       "          [-0.1627,  0.0301,  1.0000]]]),\n",
       " tensor([[[0.5043, 0.1942, 0.3015],\n",
       "          [0.2126, 0.5521, 0.2352],\n",
       "          [0.2954, 0.2105, 0.4941]],\n",
       " \n",
       "         [[0.6765, 0.1121, 0.2115],\n",
       "          [0.1072, 0.6473, 0.2454],\n",
       "          [0.1848, 0.2241, 0.5911]]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_att_layer = MatrixAttention.by_name('cosine')()\n",
    "_audio_feats = torch.randn(2, 3, 5)\n",
    "_att_map_1 = _att_layer(_audio_feats, _audio_feats)\n",
    "# att_map: (batch, seq_len, seq_len)\n",
    "_att_map_2 = masked_softmax(_att_map_1, None)\n",
    "_audio_feats, _att_map_1, _att_map_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2310)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.log(torch.FloatTensor([[1/6, 1/6, 2/3]])), torch.FloatTensor([[1/3, 1/3, 1/3]]), reduction='batchmean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.log(torch.FloatTensor([[1/3, 1/3, 1/3]])), torch.FloatTensor([[1/3, 1/3, 1/3]]), reduction='batchmean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4055)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.log(torch.FloatTensor([[1/6, 1/6, 2/3]])), torch.FloatTensor([[0, 0, 1]]), reduction='batchmean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0986)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.log(torch.FloatTensor([[1/3, 1/3, 1/3]])), torch.FloatTensor([[0, 0, 1]]), reduction='batchmean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.kl_div(torch.log(torch.FloatTensor([[1/3, 1/3, 1/3]])), torch.FloatTensor([[0, 0, 0]]), reduction='batchmean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/Users/mac/Desktop/syt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/Users/mac/Desktop/syt2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
